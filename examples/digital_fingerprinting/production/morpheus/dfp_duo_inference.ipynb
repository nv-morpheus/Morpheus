{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - DUO Inference\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs inference on Duo authentication logs. The goal is to use the pretrained models generated in the Duo Training notebook to generate anomaly scores for each log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please see the coresponding DFP training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"./morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import logging\n",
    "import os\n",
    "import typing\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import click\n",
    "import mlflow\n",
    "from dfp.stages.dfp_file_batcher_stage import DFPFileBatcherStage\n",
    "from dfp.stages.dfp_file_to_df import DFPFileToDataFrameStage\n",
    "from dfp.stages.dfp_inference_stage import DFPInferenceStage\n",
    "from dfp.stages.dfp_mlflow_model_writer import DFPMLFlowModelWriterStage\n",
    "from dfp.stages.dfp_postprocessing_stage import DFPPostprocessingStage\n",
    "from dfp.stages.dfp_preprocessing_stage import DFPPreprocessingStage\n",
    "from dfp.stages.dfp_rolling_window_stage import DFPRollingWindowStage\n",
    "from dfp.stages.dfp_split_users_stage import DFPSplitUsersStage\n",
    "from dfp.stages.dfp_training import DFPTraining\n",
    "from dfp.stages.multi_file_source import MultiFileSource\n",
    "from dfp.utils.column_info import BoolColumn\n",
    "from dfp.utils.column_info import ColumnInfo\n",
    "from dfp.utils.column_info import CustomColumn\n",
    "from dfp.utils.column_info import DataFrameInputSchema\n",
    "from dfp.utils.column_info import DateTimeColumn\n",
    "from dfp.utils.column_info import IncrementColumn\n",
    "from dfp.utils.column_info import RenameColumn\n",
    "from dfp.utils.column_info import StringCatColumn\n",
    "from dfp.utils.column_info import create_increment_col\n",
    "from dfp.utils.file_utils import date_extractor\n",
    "from dfp.utils.file_utils import iso_date_regex\n",
    "\n",
    "from morpheus._lib.file_types import FileTypes\n",
    "from morpheus.cli.utils import get_package_relative_file\n",
    "from morpheus.cli.utils import load_labels_file\n",
    "from morpheus.config import Config\n",
    "from morpheus.config import ConfigAutoEncoder\n",
    "from morpheus.config import CppConfig\n",
    "from morpheus.pipeline import LinearPipeline\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.stages.output.write_to_file_stage import WriteToFileStage\n",
    "from morpheus.utils.logger import configure_logging\n",
    "from morpheus.utils.logger import get_log_levels\n",
    "from morpheus.utils.logger import parse_log_level\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The following options significantly alter the functionality of the pipeline. These options are separated from the individual stage options since they may effect more than one stage. Additionally, the matching python script to this notebook, `dfp_duo_pipeline.py`, configures these options via command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name | Type | Description |\n",
    "| --- | --- | :-- |\n",
    "| `train_users` | One of `[\"none\"]` | For inference, this option should always be `\"none\"` |\n",
    "| `skip_users` | List of strings | Any user in this list will be dropped from the pipeline. Useful for debugging to remove automated accounts with many logs. |\n",
    "| `cache_dir` | string | The location to store cached files. To aid with development and reduce bandwidth, the Morpheus pipeline will cache data from several stages of the pipeline. This option configures the location for those caches. |\n",
    "| `input_files` | List of strings | List of files to process. Can specificy multiple arguments for multiple files. Also accepts glob (\\*) wildcards and schema prefixes such as `s3://`. For example, to make a local cache of an s3 bucket, use `filecache::s3://mybucket/*`. See `fsspec` documentation for list of possible options. |\n",
    "| `model_name_formatter` | string | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage`. Available keyword arguments: `user_id`, `user_md5`. |\n",
    "| `experiment_name_formatter` | string | A format string (without the `f`) that will be used when creating an experiment in ML Flow. Available keyword arguments: `user_id`, `user_md5`, `reg_model_name`. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global options\n",
    "train_users = \"none\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\"\n",
    "\n",
    "# Input files to read from\n",
    "input_files = [\n",
    "    \"/work/examples/data/dfp/duo/duotest_pt1.json\",\n",
    "    \"/work/examples/data/dfp/duo/duotest_pt2.json\",\n",
    "    \"/work/examples/data/dfp/duo/duotest_pt3.json\",\n",
    "    \"/work/examples/data/dfp/duo/duotest_pt4.json\"\n",
    "]\n",
    "\n",
    "# The format to use for models\n",
    "model_name_formatter = \"DFP-duo-{user_id}\"\n",
    "\n",
    "# === Derived Options ===\n",
    "# To include the generic, we must be training all or generic\n",
    "include_generic = train_users == \"all\" or train_users == \"generic\"\n",
    "\n",
    "# To include individual, we must be either training or inferring\n",
    "include_individual = train_users != \"generic\"\n",
    "\n",
    "# None indicates we arent training anything\n",
    "is_training = train_users != \"none\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc24c9-c85e-4977-a348-692c8f0aceaa",
   "metadata": {},
   "source": [
    "### Global Config Object\n",
    "Before creating the pipeline, we need to setup logging and set the parameters for the Morpheus config object. This config object is responsible for the following:\n",
    " - Indicating whether to use C++ or Python stages\n",
    "    - C++ stages are not supported for the DFP pipeline. This should always be `False`\n",
    " - Setting the number of threads to use in the pipeline. Defaults to the thread count of the OS.\n",
    " - Sets the feature column names that will be used in model training\n",
    "    - This option allows extra columns to be used in the pipeline that will not be part of the training algorithm.\n",
    "    - The final features that the model will be trained on will be an intersection of this list with the log columns.\n",
    " - The column name that indicates the user's unique identifier\n",
    "    - It is required for DFP to have a user ID column\n",
    " - The column name that indicates the timestamp for the log\n",
    "    - It is required for DFP to know when each log occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Morpheus logger\n",
    "configure_logging(log_level=logging.DEBUG)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "CppConfig.set_should_use_cpp(False)\n",
    "\n",
    "config.num_threads = os.cpu_count()\n",
    "\n",
    "config.ae = ConfigAutoEncoder()\n",
    "\n",
    "config.ae.feature_columns = [\n",
    "    'accessdevicebrowser', 'accessdeviceos', 'device', 'result', 'reason', 'logcount', \"locincrement\"\n",
    "]\n",
    "config.ae.userid_column_name = \"username\"\n",
    "config.ae.timestamp_column_name = \"timestamp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the column names to ensure all data is uniform\n",
    "source_column_info = [\n",
    "    DateTimeColumn(name=config.ae.timestamp_column_name, dtype=datetime, input_name=\"timestamp\"),\n",
    "    RenameColumn(name=config.ae.userid_column_name, dtype=str, input_name=\"user.name\"),\n",
    "    RenameColumn(name=\"accessdevicebrowser\", dtype=str, input_name=\"access_device.browser\"),\n",
    "    RenameColumn(name=\"accessdeviceos\", dtype=str, input_name=\"access_device.os\"),\n",
    "    StringCatColumn(name=\"location\",\n",
    "                    dtype=str,\n",
    "                    input_columns=[\n",
    "                        \"access_device.location.city\",\n",
    "                        \"access_device.location.state\",\n",
    "                        \"access_device.location.country\"\n",
    "                    ],\n",
    "                    sep=\", \"),\n",
    "    RenameColumn(name=\"authdevicename\", dtype=str, input_name=\"auth_device.name\"),\n",
    "    BoolColumn(name=\"result\",\n",
    "                dtype=bool,\n",
    "                input_name=\"result\",\n",
    "                true_values=[\"success\", \"SUCCESS\"],\n",
    "                false_values=[\"denied\", \"DENIED\", \"FRAUD\"]),\n",
    "    ColumnInfo(name=\"reason\", dtype=str),\n",
    "    # CustomColumn(name=\"user.groups\", dtype=str, process_column_fn=partial(column_listjoin, col_name=\"user.groups\"))\n",
    "]\n",
    "\n",
    "source_schema = DataFrameInputSchema(json_columns=[\"access_device\", \"application\", \"auth_device\", \"user\"],\n",
    "                                        column_info=source_column_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7a0cb0a-e65a-444a-a06c-a4525d543790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing schema\n",
    "preprocess_column_info = [\n",
    "    ColumnInfo(name=config.ae.timestamp_column_name, dtype=datetime),\n",
    "    ColumnInfo(name=config.ae.userid_column_name, dtype=str),\n",
    "    ColumnInfo(name=\"accessdevicebrowser\", dtype=str),\n",
    "    ColumnInfo(name=\"accessdeviceos\", dtype=str),\n",
    "    ColumnInfo(name=\"authdevicename\", dtype=str),\n",
    "    ColumnInfo(name=\"result\", dtype=bool),\n",
    "    ColumnInfo(name=\"reason\", dtype=str),\n",
    "    # Derived columns\n",
    "    IncrementColumn(name=\"logcount\",\n",
    "                    dtype=int,\n",
    "                    input_name=config.ae.timestamp_column_name,\n",
    "                    groupby_column=config.ae.userid_column_name),\n",
    "    CustomColumn(name=\"locincrement\",\n",
    "                    dtype=int,\n",
    "                    process_column_fn=partial(create_increment_col, column_name=\"location\")),\n",
    "]\n",
    "\n",
    "preprocess_schema = DataFrameInputSchema(column_info=preprocess_column_info, preserve_columns=[\"_batch_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`MultiFileSource`)\n",
    "\n",
    "This pipeline read input logs from one or more input files. This source stage will read all specified log files, combine them into a single `DataFrame`, and pass it into the pipeline. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any files to read into the pipeline. All files will be combined into a single `DataFrame` |\n",
    "\n",
    "### File Batcher Stage (`DFPFileBatcherStage`)\n",
    "\n",
    "To improve performance, multiple small input files can be batched together into a single DataFrame for processing. This stage is responsible for determining the timestamp of input files, grouping input files into batches by time, and sending the batches to be processed into a single DataFrame. Batches of files that have been seen before will be loaded from cache resulting in increased performance. For example, when performaing a 60 day training run, 59 days can be cached with a period of `\"D\"` and retraining once per day.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `period` | `str` | `\"D\"` | The period to create batches. See `pandas` windowing frequency documentation for available options.  |\n",
    "| `date_conversion_func` | Function of `typing.Callable[[fsspec.core.OpenFile], datetime]` | | A callback which is responsible for determining the date for a specified file. |\n",
    "\n",
    "### File to DataFrame Stage (`DFPFileToDataFrameStage`)\n",
    "\n",
    "After files have been batched into groups, this stage is responsible for reading the files and converting into a DataFrame. The specified input schema converts the raw DataFrame into one suitable for caching and processing. Any columns that are not needed should be excluded from the schema.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `schema` | `DataFrameInputSchema` | | After the raw `DataFrame` is read from each file, this schema will be applied to ensure a consisten output from the source. |\n",
    "| `file_type` | `FileTypes` | `FileTypes.Auto` | Allows overriding the file type. When set to `Auto`, the file extension will be used. Options are `CSV`, `JSON`, `Auto`. |\n",
    "| `parser_kwargs` | `dict` | `{}` | This dictionary will be passed to the `DataFrame` parser class. Allows for customization of log parsing. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write cached input files to. |\n",
    "\n",
    "### Split Users Stage (`DFPSplitUsersStage`)\n",
    "\n",
    "Once the input logs have been read into a `DataFrame`, this stage is responsible for breaking that single `DataFrame` with many users into multiple `DataFrame`s for each user. This is also where the pipeline chooses whether to train individual users or the generic user (or both).\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `include_generic` | `bool` | | Whether or not to combine all user logs into a single `DataFrame` with the username 'generic_user' |\n",
    "| `include_individual` | `bool` | | Whether or not to output individual `DataFrame` objects for each user |\n",
    "| `skip_users` | List of `str` | `[]` | Any users to remove from the `DataFrame`. Useful for debugging to remove automated accounts with many logs. Mutually exclusive with `only_users`. |\n",
    "| `only_users` | List of `str` | `[]` | Only allow these users in the final `DataFrame`. Useful for debugging to focus on specific users. Mutually exclusive with `skip_users`. |\n",
    "\n",
    "### Rolling Window Stage (`DFPRollingWindowStage`)\n",
    "\n",
    "The Rolling Window Stage performs several key pieces of functionality for DFP.\n",
    "1. This stage keeps a moving window of logs on a per user basis\n",
    "   1. These logs are saved to disk to reduce memory requirements between logs from the same user\n",
    "1. It only emits logs when the window history requirements are met\n",
    "   1. Until all of the window history requirements are met, no messages will be sent to the rest of the pipeline.\n",
    "   1. See the below options for configuring the window history requirements\n",
    "1. It repeats the necessary logs to properly calculate log dependent features.\n",
    "   1. To support all column feature types, incoming log messages can be combined with existing history and sent to downstream stages.\n",
    "   1. For example, to calculate a feature that increments a counter for the number of logs a particular user has generated in a single day, we must have the user's log history for the past 24 hours. To support this, this stage will combine new logs with existing history into a single `DataFrame`.\n",
    "   1. It is the responsibility of downstream stages to distinguish between new logs and existing history.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `min_history` | `int` | `1` | The minimum number of logs a user must have before emitting any messages. Logs below this threshold will be saved to disk. |\n",
    "| `min_increment` | `int` or `str` | `0` | Once the min history requirement is met, this stage must receive `min_increment` *new* logs before emmitting another message. Logs received before this threshold is met will be saved to disk. Can be specified as an integer count or a string duration. |\n",
    "| `max_history` | `int` or `str` | `\"1d\"` | Once `min_history` and `min_increment` requirements have been met, this puts an upper bound on the maximum number of messages to forward into the pipeline and also the maximum amount of messages to retain in the history. Can be specified as an integer count or a string duration. |\n",
    "| `cache_dir` | `str` | `./.cache/dfp` | The location to write log history to disk. |\n",
    "\n",
    "### Preprocessing Stage (`DFPPreprocessingStage`)\n",
    "\n",
    "This stage performs the final, row dependent, feature calculations as specified by the input schema object. Once calculated, this stage can forward on all received logs, or optionally can only forward on new logs, removing any history information.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `input_schema` | `DataFrameInputSchema` | | The final, row dependent, schema to apply to the incoming columns |\n",
    "| `only_new_batches` | `bool` | | Whether or not to foward on all received logs, or just new logs. |\n",
    "\n",
    "### Inference Stage (`DFPInference`)\n",
    "\n",
    "This stage performs several tasks to aid in performing inference. This stage will:\n",
    "1. Download models as needed from MLFlow\n",
    "1. Cache previously downloaded models to improve performance\n",
    "   1. Models in the cache will be periodically refreshed from MLFlow at a configured rate\n",
    "1. Perform inference using the downloaded model\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `model_name_formatter` | `str` | `\"\"` | A format string to use when building the model name. The model name is constructed by calling `model_name_formatter.format(user_id=user_id)`. For example, with `model_name_formatter=\"my_model-{user_id}\"` and a user ID of `\"first:last\"` would result in the model name of `\"my_model-first:last\"`. This should match the value used in `DFPMLFlowModelWriterStage` |\n",
    "\n",
    "### Post Processing Stage (`DFPPostprocessingStage`)\n",
    "\n",
    "This stage filters the output from the inference stage for any anomalous messages. Logs which exceed the specified Z-Score will be passed onto the next stage. All remaining logs which are below the threshold will be dropped.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `z_score_threshold` | `float` | `2.0` | The Z-Score used to separate anomalous logs from normal logs. All normal logs will be filterd out and anomalous logs will be passed on. |\n",
    "\n",
    "### Write to File Stage (`WriteToFileStage`)\n",
    "\n",
    "This final stage will write all received messages to a single output file in either CSV or JSON format.\n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filename` | `str` | | The file to write anomalous log messages to. |\n",
    "| `overwrite` | `bool` | `False` | If the file specified in `filename` already exists, it will be overwritten if this option is set to `True` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Registering Pipeline====\u001b[0m\n",
      "====Registering Pipeline Complete!====\u001b[0m\n",
      "====Starting Pipeline====\u001b[0m\n",
      "====Pipeline Started====\u001b[0m\n",
      "====Building Pipeline====\u001b[0m\n",
      "Added source: <from-multi-file-0; MultiFileSource(input_schema=DataFrameInputSchema(json_columns=['access_device', 'application', 'auth_device', 'user'], column_info=[RenameColumn(name='accessdevicebrowser', dtype=<class 'str'>, input_name='access_device.browser'), RenameColumn(name='accessdeviceos', dtype=<class 'str'>, input_name='access_device.os'), RenameColumn(name='locationcity', dtype=<class 'str'>, input_name='auth_device.location.city'), RenameColumn(name='device', dtype=<class 'str'>, input_name='auth_device.name'), BoolColumn(name='result', dtype=<class 'bool'>, input_name='result', value_map={'success': True, 'SUCCESS': True, 'denied': False, 'DENIED': False, 'FRAUD': False}), RenameColumn(name='reason', dtype=<class 'str'>, input_name='reason'), RenameColumn(name='username', dtype=<class 'str'>, input_name='user.name'), RenameColumn(name='timestamp', dtype=<class 'datetime.datetime'>, input_name='timestamp')], preserve_columns=None), filenames=['/work/examples/data/dfp/duo/duotest_pt1.json', '/work/examples/data/dfp/duo/duotest_pt2.json', '/work/examples/data/dfp/duo/duotest_pt3.json', '/work/examples/data/dfp/duo/duotest_pt4.json'], file_type=FileTypes.Auto, parser_kwargs={'lines': False, 'orient': 'records'})>\n",
      "  └─> cudf.DataFrame\u001b[0m\n",
      "Added stage: <dfp-split-users-1; DFPSplitUsersStage(include_generic=False, include_individual=True, skip_users=[])>\n",
      "  └─ cudf.DataFrame -> dfp.DFPMessageMeta\u001b[0m\n",
      "Added stage: <dfp-rolling-window-2; DFPRollingWindowStage(min_history=1, min_increment=0, max_history=1d, cache_dir=./.cache/dfp)>\n",
      "  └─ dfp.DFPMessageMeta -> dfp.MultiDFPMessage\u001b[0m\n",
      "Added stage: <dfp-preproc-3; DFPPreprocessingStage(input_schema=DataFrameInputSchema(json_columns=[], column_info=[RenameColumn(name='accessdevicebrowser', dtype=<class 'str'>, input_name='accessdevicebrowser'), RenameColumn(name='accessdeviceos', dtype=<class 'str'>, input_name='accessdeviceos'), RenameColumn(name='device', dtype=<class 'str'>, input_name='device'), RenameColumn(name='result', dtype=<class 'bool'>, input_name='result'), RenameColumn(name='reason', dtype=<class 'str'>, input_name='reason'), CustomColumn(name='logcount', dtype=<class 'int'>, process_column_fn=<function column_logcount at 0x7f43700068b0>), CustomColumn(name='locincrement', dtype=<class 'int'>, process_column_fn=<function column_locincrement at 0x7f4370006670>), RenameColumn(name='username', dtype=<class 'str'>, input_name='username'), RenameColumn(name='timestamp', dtype=<class 'datetime.datetime'>, input_name='timestamp')], preserve_columns=re.compile('(_batch_id)')), return_format=data, only_new_batches=True)>\n",
      "  └─ dfp.MultiDFPMessage -> dfp.MultiDFPMessage\u001b[0m\n",
      "Added stage: <dfp-inference-4; DFPInferenceStage(model_name_formatter=AE-duo-{user_id})>\n",
      "  └─ dfp.MultiDFPMessage -> morpheus.MultiAEMessage\u001b[0m\n",
      "Added stage: <dfp-postproc-5; DFPPostprocessingStage(z_score_threshold=3.0)>\n",
      "  └─ morpheus.MultiAEMessage -> dfp.DFPMessageMeta\u001b[0m\n",
      "Added stage: <to-file-6; WriteToFileStage(filename=dfp_detections.csv, overwrite=True, file_type=FileTypes.Auto, include_index_col=True)>\n",
      "  └─ dfp.DFPMessageMeta -> dfp.DFPMessageMeta\u001b[0m\n",
      "====Building Pipeline Complete!====\u001b[0m\n",
      "\u001b[2mStarting! Time: 1661583516.3895624\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20220827 06:58:36.361177   496 thread.cpp:138] unable to set memory policy - if using docker use: --cap-add=sys_nice to allow membind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 4000 rows\n",
      "\u001b[2mBatch split users complete. Input: 4000 rows from 2021-09-14 00:52:14 to 2022-01-24 15:23:41. Output: 10 users, rows/user min: 176, max: 700, avg: 400.00. Duration: 5.55 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for badguy in 16.03 ms. Input: 490 rows from 2021-09-14 00:52:14 to 2022-01-24 09:30:26. Output: 490 rows from 2021-09-14 00:52:14 to 2022-01-24 09:30:26\u001b[0m\n",
      "\u001b[2mPreprocessed 490 data for logs in 2021-09-14 00:52:14 to 2022-01-24 09:30:26 in 34.24859046936035 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for maliciousactor in 37.56 ms. Input: 700 rows from 2021-09-14 16:58:15 to 2022-01-24 15:23:41. Output: 700 rows from 2021-09-14 16:58:15 to 2022-01-24 15:23:41\u001b[0m\n",
      "\u001b[2mRolling window complete for usera in 24.63 ms. Input: 215 rows from 2021-10-07 10:53:23 to 2022-01-24 15:21:18. Output: 215 rows from 2021-10-07 10:53:23 to 2022-01-24 15:21:18\u001b[0m\n",
      "\u001b[2mRolling window complete for userb in 28.06 ms. Input: 457 rows from 2021-10-07 04:02:07 to 2022-01-24 15:22:10. Output: 457 rows from 2021-10-07 04:02:07 to 2022-01-24 15:22:10\u001b[0m\n",
      "\u001b[2mPreprocessed 700 data for logs in 2021-09-14 16:58:15 to 2022-01-24 15:23:41 in 52.93703079223633 ms\u001b[0m\n",
      "\u001b[2mPreprocessed 215 data for logs in 2021-10-07 10:53:23 to 2022-01-24 15:21:18 in 25.563955307006836 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for userc in 45.67 ms. Input: 500 rows from 2021-10-06 20:29:50 to 2022-01-24 15:23:00. Output: 500 rows from 2021-10-06 20:29:50 to 2022-01-24 15:23:00\u001b[0m\n",
      "\u001b[2mPreprocessed 457 data for logs in 2021-10-07 04:02:07 to 2022-01-24 15:22:10 in 42.35053062438965 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for userd in 39.30 ms. Input: 537 rows from 2021-10-07 02:25:27 to 2022-01-24 12:26:34. Output: 537 rows from 2021-10-07 02:25:27 to 2022-01-24 12:26:34\u001b[0m\n",
      "\u001b[2mRolling window complete for usere in 25.54 ms. Input: 176 rows from 2021-12-15 12:12:25 to 2022-01-24 07:56:46. Output: 176 rows from 2021-12-15 12:12:25 to 2022-01-24 07:56:46\u001b[0m\n",
      "\u001b[2mPreprocessed 500 data for logs in 2021-10-06 20:29:50 to 2022-01-24 15:23:00 in 49.77822303771973 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for userf in 35.04 ms. Input: 304 rows from 2021-10-07 10:55:23 to 2022-01-24 14:08:00. Output: 304 rows from 2021-10-07 10:55:23 to 2022-01-24 14:08:00\u001b[0m\n",
      "\u001b[2mRolling window complete for userg in 36.67 ms. Input: 275 rows from 2021-10-07 10:55:01 to 2022-01-24 15:23:25. Output: 275 rows from 2021-10-07 10:55:01 to 2022-01-24 15:23:25\u001b[0m\n",
      "\u001b[2mPreprocessed 537 data for logs in 2021-10-07 02:25:27 to 2022-01-24 12:26:34 in 71.11096382141113 ms\u001b[0m\n",
      "\u001b[2mRolling window complete for userh in 25.03 ms. Input: 346 rows from 2021-10-07 10:54:07 to 2022-01-24 15:22:10. Output: 346 rows from 2021-10-07 10:54:07 to 2022-01-24 15:22:10\u001b[0m\n",
      "\u001b[2mPreprocessed 176 data for logs in 2021-12-15 12:12:25 to 2022-01-24 07:56:46 in 23.290157318115234 ms\u001b[0m\n",
      "\u001b[2mPreprocessed 304 data for logs in 2021-10-07 10:55:23 to 2022-01-24 14:08:00 in 27.817487716674805 ms\u001b[0m\n",
      "\u001b[2mPreprocessed 275 data for logs in 2021-10-07 10:55:01 to 2022-01-24 15:23:25 in 35.3856086730957 ms\u001b[0m\n",
      "\u001b[2mPreprocessed 346 data for logs in 2021-10-07 10:54:07 to 2022-01-24 15:22:10 in 26.59463882446289 ms\u001b[0m\n",
      "\u001b[2mCompleted inference for user badguy. Model load: 4450.737953186035 ms, Model infer: 17.392873764038086 ms. Start: 2021-09-14 00:52:14, End: 2022-01-24 09:30:26\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user badguy in 6.493568420410156 ms. Event count: 12. Start: 2021-09-14 00:52:14, End: 2022-01-24 09:30:26\u001b[0m\n",
      "\u001b[2mCompleted inference for user maliciousactor. Model load: 168.58458518981934 ms, Model infer: 14.253854751586914 ms. Start: 2021-09-14 16:58:15, End: 2022-01-24 15:23:41\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user maliciousactor in 6.427764892578125 ms. Event count: 0. Start: 2021-09-14 16:58:15, End: 2022-01-24 15:23:41\u001b[0m\n",
      "\u001b[2mCompleted inference for user usera. Model load: 162.7488136291504 ms, Model infer: 13.43989372253418 ms. Start: 2021-10-07 10:53:23, End: 2022-01-24 15:21:18\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user usera in 4.646778106689453 ms. Event count: 0. Start: 2021-10-07 10:53:23, End: 2022-01-24 15:21:18\u001b[0m\n",
      "\u001b[2mCompleted inference for user userb. Model load: 159.15226936340332 ms, Model infer: 14.02425765991211 ms. Start: 2021-10-07 04:02:07, End: 2022-01-24 15:22:10\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userb in 6.743431091308594 ms. Event count: 1. Start: 2021-10-07 04:02:07, End: 2022-01-24 15:22:10\u001b[0m\n",
      "\u001b[2mCompleted inference for user userc. Model load: 162.69850730895996 ms, Model infer: 14.590740203857422 ms. Start: 2021-10-06 20:29:50, End: 2022-01-24 15:23:00\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userc in 6.506443023681641 ms. Event count: 3. Start: 2021-10-06 20:29:50, End: 2022-01-24 15:23:00\u001b[0m\n",
      "\u001b[2mCompleted inference for user userd. Model load: 195.6191062927246 ms, Model infer: 14.337778091430664 ms. Start: 2021-10-07 02:25:27, End: 2022-01-24 12:26:34\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userd in 14.258861541748047 ms. Event count: 24. Start: 2021-10-07 02:25:27, End: 2022-01-24 12:26:34\u001b[0m\n",
      "\u001b[2mCompleted inference for user usere. Model load: 0.6737709045410156 ms, Model infer: 24.237871170043945 ms. Start: 2021-12-15 12:12:25, End: 2022-01-24 07:56:46\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user usere in 4.172325134277344 ms. Event count: 0. Start: 2021-12-15 12:12:25, End: 2022-01-24 07:56:46\u001b[0m\n",
      "\u001b[2mCompleted inference for user userf. Model load: 163.68651390075684 ms, Model infer: 14.430522918701172 ms. Start: 2021-10-07 10:55:23, End: 2022-01-24 14:08:00\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userf in 13.100862503051758 ms. Event count: 9. Start: 2021-10-07 10:55:23, End: 2022-01-24 14:08:00\u001b[0m\n",
      "\u001b[2mCompleted inference for user userg. Model load: 1.8045902252197266 ms, Model infer: 23.801803588867188 ms. Start: 2021-10-07 10:55:01, End: 2022-01-24 15:23:25\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userg in 5.267620086669922 ms. Event count: 0. Start: 2021-10-07 10:55:01, End: 2022-01-24 15:23:25\u001b[0m\n",
      "\u001b[2mCompleted inference for user userh. Model load: 191.9386386871338 ms, Model infer: 15.669107437133789 ms. Start: 2021-10-07 10:54:07, End: 2022-01-24 15:22:10\u001b[0m\n",
      "\u001b[2mCompleted postprocessing for user userh in 8.036375045776367 ms. Event count: 45. Start: 2021-10-07 10:54:07, End: 2022-01-24 15:22:10\u001b[0m\n",
      "====Pipeline Complete====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create a linear pipeline object\n",
    "pipeline = LinearPipeline(config)\n",
    "\n",
    "# Source stage\n",
    "pipeline.set_source(MultiFileSource(config, filenames=input_files))\n",
    "\n",
    "# Batch files into buckets by time. Use the default ISO date extractor from the filename\n",
    "pipeline.add_stage(\n",
    "    DFPFileBatcherStage(config,\n",
    "                        period=\"D\",\n",
    "                        date_conversion_func=functools.partial(date_extractor, filename_regex=iso_date_regex)))\n",
    "\n",
    "# Output is S3 Buckets. Convert to DataFrames. This caches downloaded S3 data\n",
    "pipeline.add_stage(\n",
    "    DFPFileToDataFrameStage(config,\n",
    "                            schema=source_schema,\n",
    "                            file_type=FileTypes.JSON,\n",
    "                            parser_kwargs={\n",
    "                                \"lines\": False, \"orient\": \"records\"\n",
    "                            },\n",
    "                            cache_dir=cache_dir))\n",
    "\n",
    "\n",
    "# This will split users or just use one single user\n",
    "pipeline.add_stage(\n",
    "    DFPSplitUsersStage(config,\n",
    "                        include_generic=include_generic,\n",
    "                        include_individual=include_individual,\n",
    "                        skip_users=skip_users))\n",
    "\n",
    "# Next, have a stage that will create rolling windows\n",
    "pipeline.add_stage(\n",
    "    DFPRollingWindowStage(\n",
    "        config,\n",
    "        min_history=300 if is_training else 1,\n",
    "        min_increment=300 if is_training else 0,\n",
    "        # For inference, we only ever want 1 day max\n",
    "        max_history=\"60d\" if is_training else \"1d\",\n",
    "        cache_dir=cache_dir))\n",
    "\n",
    "# Output is UserMessageMeta -- Cached frame set\n",
    "pipeline.add_stage(DFPPreprocessingStage(config, input_schema=preprocess_schema, only_new_batches=not is_training))\n",
    "\n",
    "# Perform inference on the preprocessed data\n",
    "pipeline.add_stage(DFPInferenceStage(config, model_name_formatter=model_name_formatter))\n",
    "\n",
    "# Filter for only the anomalous logs\n",
    "pipeline.add_stage(DFPPostprocessingStage(config, z_score_threshold=2.0))\n",
    "\n",
    "# Write all anomalies to a CSV file\n",
    "pipeline.add_stage(WriteToFileStage(config, filename=\"dfp_detections_duo.csv\", overwrite=True))\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline._do_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0cf6b-8255-4d90-b67c-151518c7423b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:morpheus] *",
   "language": "python",
   "name": "conda-env-morpheus-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e26783b24f020aa0bcaa00e6ba122db5d0e3da2d892d80be664969895e06a7e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
