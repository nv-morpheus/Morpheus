{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - DUO Integrated Training\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs both training and inference on Duo authentication logs. The goal is to train an autoencoder PyTorch model to recogize the patterns of users in the sample data. The model will then be used by another fork (inference) in the pipeline to generate anomaly scores for each individual log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP integrated training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import typing\n",
    "import cudf\n",
    "from datetime import datetime\n",
    "\n",
    "# When segment modules are imported, they're added to the module registry. \n",
    "# To avoid flake8 warnings about unused code, the noqa flag is used during import.\n",
    "from dfp import modules  # noqa: F401\n",
    "from morpheus import modules  # noqa: F401\n",
    "from morpheus import loaders  # noqa: F401\n",
    "from dfp.utils.config_generator import ConfigGenerator\n",
    "from dfp.utils.config_generator import generate_ae_config\n",
    "from dfp.utils.dfp_arg_parser import DFPArgParser\n",
    "from dfp.utils.schema_utils import Schema\n",
    "from dfp.utils.schema_utils import SchemaBuilder\n",
    "\n",
    "from morpheus.config import Config\n",
    "from morpheus.pipeline.pipeline import Pipeline\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.stages.general.multiport_modules_stage import MultiPortModulesStage\n",
    "from morpheus.stages.input.control_message_file_source_stage import ControlMessageFileSourceStage\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The pipeline's functionality can be significantly altered by the following options, which are utilized across the entire pipeline. However, module-specific options also exist. The matching Python script for this notebook, `dfp_integrated_training_batch_pipeline.py`, configures these options through command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name                   | Type                                       | Description                                                                                                                                                                                                                                                                            | Default Value |\n",
    "|------------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `source`            | One of `[\"duo\", \"azure\"]`           | Indicates what type of logs are going to be used in the workload.                                                                                                                                                                                                            | -             |\n",
    "| `train_users`    | One of `[\"all\", \"generic\", \"individual\"]` | Indicates whether or not to train per user or a generic model for all users. Selecting none runs the inference pipeline.                                                                                                                                                                    | -             |\n",
    "| `skip_user`        | List of strings                                       | User IDs to skip. Mutually exclusive with `only_user`.                                                                                                                                                                                                                                               | -             |\n",
    "| `only_user`        | List of strings                                       | Only users specified by this option will be included. Mutually exclusive with `skip_user`.                                                                                                                                                                                                 | -             |\n",
    "| `start_time`       | `str`                                         | The start of the time window, if undefined start_date will be `now()-duration`.                                                                                                                                                                                                  | -             |\n",
    "| `duration`         | `str`                                         | The training duration to run starting from `start_time`.                                                                                                                                                                                                                                              | -             |\n",
    "| `cache_dir`        | `str`                                         | The location to cache data such as S3 downloads and pre-processed data.                                                                                                                                                                                                                    | -             |\n",
    "| `log_level`        | `str`                                         | Specify the logging level to use.                                                                                                                                                                                                                                                                    | `info`        |\n",
    "| `sample_rate_s`    | `int`                                         | Minimum time step, in milliseconds, between object logs.                                                                                                                                                                                                                                           | `0`        |\n",
    "| `silence_monitors`    | `bool`                                         | Controls whether monitors will be verbose logs.                                                                                                                                                                                                                                           | `False`        |\n",
    "| `tracking_uri`     | `str`                                         | The MLflow tracking URI to connect to the tracking backend.                                                                                                                                                                                                                                    | -             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "source = \"duo\"\n",
    "\n",
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.strptime(\"2022-08-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# Duration\n",
    "duration = \"60d\"\n",
    "\n",
    "# Smaple rate secs\n",
    "sample_rate_s = 0\n",
    "\n",
    "# MLFLow tracking uri\n",
    "tracking_uri = \"http://mlflow:5000\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_user: typing.List[str] = []\n",
    "\n",
    "# Only users\n",
    "only_user: typing.List[str] = []\n",
    "\n",
    "# Setting Log level\n",
    "log_level = logging.WARN\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"/workspace/.cache/dfp\"\n",
    "\n",
    "# Silence monitors\n",
    "silence_monitors = True\n",
    "\n",
    "# Control messages as input files\n",
    "load_train_only_input_files = [\n",
    "    \"./resource/duo_payload_lt.json\"\n",
    "]\n",
    "load_train_inference_input_files = [\n",
    "    \"./resource/duo_payload_lti.json\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f80a19",
   "metadata": {},
   "source": [
    "### Arguments Parser\n",
    "\n",
    "The [DFPArgParser](../../../production/morpheus/dfp/utils/dfp_arg_parser.py) class is used for parsing and storing arguments used in a  pipeline for training, generating models and inference. It has several properties and methods to transform, store and access the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_arg_parser = DFPArgParser(\n",
    "    skip_user,\n",
    "    only_user,\n",
    "    start_time,\n",
    "    log_level,\n",
    "    cache_dir,\n",
    "    sample_rate_s,\n",
    "    duration,\n",
    "    source,\n",
    "    tracking_uri,\n",
    "    silence_monitors,\n",
    "    train_users\n",
    ")\n",
    "\n",
    "# Initalize parser\n",
    "dfp_arg_parser.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global config object for the pipeline\n",
    "config: Config = generate_ae_config(\n",
    "    source,\n",
    "    userid_column_name=\"username\",\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    use_cpp=True,\n",
    ")\n",
    "\n",
    "# Construct the dataframe Schema which is used to normalize incoming duo logs\n",
    "schema_builder = SchemaBuilder(config, source)\n",
    "schema: Schema = schema_builder.build_schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb42888",
   "metadata": {},
   "source": [
    "### DFP Deployment Module Configuration\n",
    "This module sets up modular Digital Fingerprinting intergated training pipeline instance.\n",
    "\n",
    "### Configurable Parameters\n",
    "\n",
    "| Parameter           | Type | Description                               | Example Value | Default Value |\n",
    "|---------------------|------|-------------------------------------------|---------------|---------------|\n",
    "| `inference_options` | dict | Options for the inference pipeline module | See Below     | `[Required]`  |\n",
    "| `training_options`  | dict | Options for the training pipeline module  | See Below     | `[Required]`  |\n",
    "\n",
    "### Training Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|---------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`           |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`    |\n",
    "| `dfencoder_options`          | dict | Options for configuring the data frame encoder | See Below            | `-`           |\n",
    "| `mlflow_writer_options`      | dict | Options for the MLflow model writer            | See Below            | `-`           |\n",
    "| `preprocessing_options`      | dict | Options for preprocessing the data             | See Below            | `-`           |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`           |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column used in the data  | \"my_timestamp\"       | `timestamp`   |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`           |\n",
    "\n",
    "### Inference Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value  |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|----------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`            |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`     |\n",
    "| `detection_criteria`         | dict | Criteria for filtering detections              | See Below            | `-`            |\n",
    "| `fallback_username`          | str  | User ID to use if user ID not found            | \"generic_user\"       | `generic_user` |\n",
    "| `inference_options`          | dict | Options for the inference module               | See Below            | `-`            |\n",
    "| `model_name_formatter`       | str  | Format string for the model name               | \"model_{timestamp}\"  | `[Required]`   |\n",
    "| `num_output_ports`           | int  | Number of output ports for the module          | 3                    | `-`            |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column in the input data | \"timestamp\"          | `timestamp`    |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`            |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`            |\n",
    "| `write_to_file_options`      | dict | Options for writing the detections to a file   | See Below            | `-`            |\n",
    "\n",
    "### `batching_options`\n",
    "\n",
    "| Key                      | Type            | Description                         | Example Value                               | Default Value              |\n",
    "|--------------------------|-----------------|-------------------------------------|---------------------------------------------|----------------------------|\n",
    "| `end_time`               | datetime/string | Endtime of the time window          | \"2023-03-14T23:59:59\"                       | `None`                     |\n",
    "| `iso_date_regex_pattern` | string          | Regex pattern for ISO date matching | \"\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\" | `<iso_date_regex_pattern>` |\n",
    "| `parser_kwargs`          | dictionary      | Additional arguments for the parser | {}                                          | `{}`                       |\n",
    "| `period`                 | string          | Time period for grouping files      | \"1d\"                                        | `D`                        |\n",
    "| `sampling_rate_s`        | integer         | Sampling rate in seconds            | 0                                          | `None`                       |\n",
    "| `start_time`             | datetime/string | Start time of the time window       | \"2023-03-01T00:00:00\"                       | `None`                     |\n",
    "\n",
    "### `dfencoder_options`\n",
    "\n",
    "| Parameter         | Type  | Description                            | Example Value                                                                                                                                                                                                                                                 | Default Value |\n",
    "|-------------------|-------|----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `feature_columns` | list  | List of feature columns to train on    | [\"column1\", \"column2\", \"column3\"]                                                                                                                                                                                                                             | `-`           |\n",
    "| `epochs`          | int   | Number of epochs to train for          | 50                                                                                                                                                                                                                                                            | `-`           |\n",
    "| `model_kwargs`    | dict  | Keyword arguments to pass to the model | {\"encoder_layers\": [64, 32], \"decoder_layers\": [32, 64], \"activation\": \"relu\", \"swap_p\": 0.1, \"lr\": 0.001, \"lr_decay\": 0.9, \"batch_size\": 32, \"verbose\": 1, \"optimizer\": \"adam\", \"scalar\": \"min_max\", \"min_cats\": 10, \"progress_bar\": false, \"device\": \"cpu\"} | `-`           |\n",
    "| `validation_size` | float | Size of the validation set             | 0.1                                                                                                                                                                                                                                                           | `-`           |\n",
    "\n",
    "### `monitor_options`\n",
    "\n",
    "| Key                         | Type    | Description                                                | Example Value | Default Value |\n",
    "| ----------------------------|---------|------------------------------------------------------------|---------------|---------------|\n",
    "| `description`               | string  | Name to show for this Monitor Stage in the console window  | \"Progress\"    | `Progress`    |\n",
    "| `silence_monitors`          | bool    | Silence the monitors on the console                        | See Below     | `None`        |\n",
    "| `smoothing`                 | float   | Smoothing parameter to determine how much the throughput should be averaged | 0.01 | `0.05` |\n",
    "| `unit`                      | string  | Units to show in the rate value                             | \"messages\"    | `messages`    |\n",
    "| `delayed_start`             | bool    | When delayed_start is enabled, the progress bar will not be shown until the first message is received. Otherwise, the progress bar is shown on pipeline startup and will begin timing immediately. In large pipelines, this option may be desired to give a more accurate timing. | True  | `False`   |\n",
    "| `determine_count_fn_schema` | string  | Custom function for determining the count in a message      | \"Progress\"    | `Progress`    |\n",
    "| `log_level`                 | string  | Enable this stage when the configured log level is at `log_level` or lower. | \"DEBUG\" | `INFO` |\n",
    "\n",
    "\n",
    "### `mlflow_writer_options`\n",
    "\n",
    "| Key                         | Type       | Description                       | Example Value                 | Default Value |\n",
    "|-----------------------------|------------|-----------------------------------|-------------------------------|---------------|\n",
    "| `conda_env`                 | string     | Conda environment for the model   | \"path/to/conda_env.yml\"       | `[Required]`  |\n",
    "| `databricks_permissions`    | dictionary | Permissions for the model         | See Below                     | `None`        |\n",
    "| `experiment_name_formatter` | string     | Formatter for the experiment name | \"experiment_name_{timestamp}\" | `[Required]`  |\n",
    "| `model_name_formatter`      | string     | Formatter for the model name      | \"model_name_{timestamp}\"      | `[Required]`  |\n",
    "| `timestamp_column_name`     | string     | Name of the timestamp column      | \"timestamp\"                   | `timestamp`   |\n",
    "\n",
    "### `stream_aggregation_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                                 | Example Value | Default Value |\n",
    "|-------------------------|--------|-------------------------------------------------------------|---------------|---------------|\n",
    "| `cache_mode`            | string | The user ID to use if the user ID is not found              | \"batch\"       | `batch`       |\n",
    "| `min_history`           | int    | Minimum history to trigger a new training event             | 1             | `1`           |\n",
    "| `max_history`           | int    | Maximum history to include in a new training event          | 0             | `0`           |\n",
    "| `timestamp_column_name` | string | Name of the column containing timestamps                    | \"timestamp\"   | `timestamp`   |\n",
    "| `aggregation_span`      | string | Lookback timespan for training data in a new training event | \"60d\"         | `60d`         |\n",
    "| `cache_to_disk`         | bool   | Whether or not to cache streaming data to disk              | false         | `false`       |\n",
    "| `cache_dir`             | string | Directory to use for caching streaming data                 | \"./.cache\"    | `./.cache`    |\n",
    "\n",
    "### `user_splitting_options`\n",
    "\n",
    "| Key                     | Type | Description                                          | Example Value               | Default Value  |\n",
    "|-------------------------|------|------------------------------------------------------|-----------------------------|----------------|\n",
    "| `fallback_username`     | str  | The user ID to use if the user ID is not found       | \"generic_user\"              | `generic_user` |\n",
    "| `include_generic`       | bool | Whether to include a generic user ID in the output   | false                       | `false`        |\n",
    "| `include_individual`    | bool | Whether to include individual user IDs in the output | true                        | `false`        |\n",
    "| `only_users`            | list | List of user IDs to include; others will be excluded | [\"user1\", \"user2\", \"user3\"] | `[]`           |\n",
    "| `skip_users`            | list | List of user IDs to exclude from the output          | [\"user4\", \"user5\"]          | `[]`           |\n",
    "| `timestamp_column_name` | str  | Name of the column containing timestamps             | \"timestamp\"                 | `timestamp`    |\n",
    "| `userid_column_name`    | str  | Name of the column containing user IDs               | \"username\"                  | `username`     |\n",
    "\n",
    "### `detection_criteria`\n",
    "\n",
    "| Key          | Type  | Description                              | Example Value | Default Value |\n",
    "|--------------|-------|------------------------------------------|---------------|---------------|\n",
    "| `threshold`  | float | Threshold for filtering detections       | 0.5           | `0.5`         |\n",
    "| `field_name` | str   | Name of the field to filter by threshold | \"score\"       | `probs`       |\n",
    "\n",
    "### `inference_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                          | Example Value           | Default Value |\n",
    "|-------------------------|--------|------------------------------------------------------|-------------------------|---------------|\n",
    "| `model_name_formatter`  | string | Formatter for model names                            | \"user_{username}_model\" | `[Required]`  |\n",
    "| `fallback_username`     | string | Fallback user to use if no model is found for a user | \"generic_user\"          | `generic_user`|\n",
    "| `timestamp_column_name` | string | Name of the timestamp column                         | \"timestamp\"             | `timestamp`   |\n",
    "\n",
    "### `write_to_file_options`\n",
    "\n",
    "| Key                 | Type      | Description                              | Example Value   | Default Value    |\n",
    "|---------------------|-----------|------------------------------------------|-----------------|------------------|\n",
    "| `filename`          | string    | Path to the output file                  | \"output.csv\"    | `None`           |\n",
    "| `file_type`         | string    | Type of file to write                    | \"CSV\"           | `AUTO`           |\n",
    "| `flush`             | bool      | If true, flush the file after each write | false           | `false`          |\n",
    "| `include_index_col` | bool      | If true, include the index column        | false           | `true`           |\n",
    "| `overwrite`         | bool      | If true, overwrite the file if it exists | true            | `false`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config helper is used to generate config parameters for the DFP module\n",
    "# This will populate to the minimum configuration parameters with intelligent default values\n",
    "config_generator = ConfigGenerator(config, dfp_arg_parser, schema)\n",
    "\n",
    "dfp_deployment_module_config = config_generator.get_module_conf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`ControlMessageFileSourceStage`)\n",
    "\n",
    "This pipeline read control message definations from one or more input files. This source stage will constructs control message and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any control message defination files to read into the pipeline |\n",
    "\n",
    "### DFP Deployment Module (`MultiPortModulesStage`)\n",
    "\n",
    "MultiPortModulesStage is used to load modules that returns more than one ouptut. DFP deployment module sets up modular Digital Fingerprinting Pipeline  instance. and performs integrated training as shown in the below diagram. For more information on the options passed to this module is shown [here](../../../../../docs/source/modules/examples/digital_fingerprinting/dfp_deployment.md).\n",
    "\n",
    "\n",
    "## End-to-End Workflow Architecture\n",
    "\n",
    "![Integrated Training Pipeline](../images/dfp_integrated_training_file_pipeline.jpg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipeline():\n",
    "    # Create a pipeline object\n",
    "    pipeline = Pipeline(config)\n",
    "\n",
    "    # ControlMessage file source stage.\n",
    "    source_stage = pipeline.add_stage(ControlMessageFileSourceStage(config, filenames=input_files))\n",
    "\n",
    "    # DFP deployment (integrated training) module stage.\n",
    "    dfp_deployment_stage = pipeline.add_stage(\n",
    "        MultiPortModulesStage(config,\n",
    "                                dfp_deployment_module_config,\n",
    "                                input_port_name=\"input\",\n",
    "                                output_port_name_prefix=\"output\",\n",
    "                                num_output_ports=2))\n",
    "\n",
    "    # Connect stages with edges.\n",
    "    pipeline.add_edge(source_stage, dfp_deployment_stage)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b4b93cb",
   "metadata": {},
   "source": [
    "### Training\n",
    "To ensure a smooth deployment of the inference tasks to the pipeline, it is imperative to have at least one version of the trained model available on the MLflow server. Once the initial model training is complete, we can proceed with the publishing of the inference tasks to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0061d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_only_input_files\n",
    "\n",
    "pipeline = construct_pipeline()\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc1472a0",
   "metadata": {},
   "source": [
    "### Training and Inference\n",
    "Now that we have a trained model available in MLflow, we can begin executing both training and inference tasks in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64339676",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_inference_input_files\n",
    "\n",
    "pipeline = construct_pipeline()\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a24caa6f",
   "metadata": {},
   "source": [
    "### Inference Results\n",
    "Pipeline writes the inference results to `df_detections_duo.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "960f14ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>logcount</th>\n",
       "      <th>logcount_pred</th>\n",
       "      <th>logcount_loss</th>\n",
       "      <th>logcount_z_loss</th>\n",
       "      <th>locincrement</th>\n",
       "      <th>locincrement_pred</th>\n",
       "      <th>locincrement_loss</th>\n",
       "      <th>locincrement_z_loss</th>\n",
       "      <th>result</th>\n",
       "      <th>...</th>\n",
       "      <th>reason_pred</th>\n",
       "      <th>reason_loss</th>\n",
       "      <th>reason_z_loss</th>\n",
       "      <th>max_abs_z</th>\n",
       "      <th>mean_abs_z</th>\n",
       "      <th>z_loss_scaler_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>username</th>\n",
       "      <th>model_version</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5.698269</td>\n",
       "      <td>1.879187</td>\n",
       "      <td>0.662442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.991324</td>\n",
       "      <td>2.577614</td>\n",
       "      <td>1.317140</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>1.165407</td>\n",
       "      <td>4.262609</td>\n",
       "      <td>4.262609</td>\n",
       "      <td>2.076225</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T00:01:31.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-attacktarget:191</td>\n",
       "      <td>2023-04-29T00:49:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>5.711531</td>\n",
       "      <td>3.975875</td>\n",
       "      <td>2.250193</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.972813</td>\n",
       "      <td>2.482248</td>\n",
       "      <td>1.239232</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>1.126028</td>\n",
       "      <td>3.074252</td>\n",
       "      <td>3.976172</td>\n",
       "      <td>2.126213</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T00:40:41.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-attacktarget:191</td>\n",
       "      <td>2023-04-29T00:49:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>15</td>\n",
       "      <td>5.698916</td>\n",
       "      <td>5.006695</td>\n",
       "      <td>3.030799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.970755</td>\n",
       "      <td>2.471756</td>\n",
       "      <td>1.230660</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>1.124552</td>\n",
       "      <td>3.029720</td>\n",
       "      <td>4.107398</td>\n",
       "      <td>2.296612</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T00:48:22.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-attacktarget:191</td>\n",
       "      <td>2023-04-29T00:49:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>5.684619</td>\n",
       "      <td>6.158211</td>\n",
       "      <td>3.902803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.968191</td>\n",
       "      <td>2.458717</td>\n",
       "      <td>1.220008</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>1.124198</td>\n",
       "      <td>3.019036</td>\n",
       "      <td>4.208632</td>\n",
       "      <td>2.498048</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T00:53:18.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-attacktarget:191</td>\n",
       "      <td>2023-04-29T00:49:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>5.670151</td>\n",
       "      <td>7.429033</td>\n",
       "      <td>4.865154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.966020</td>\n",
       "      <td>2.447701</td>\n",
       "      <td>1.211009</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>1.124767</td>\n",
       "      <td>3.036199</td>\n",
       "      <td>4.865154</td>\n",
       "      <td>2.720832</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T00:54:48.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-attacktarget:191</td>\n",
       "      <td>2023-04-29T00:49:13Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>661</td>\n",
       "      <td>507</td>\n",
       "      <td>17.697769</td>\n",
       "      <td>18474.744140</td>\n",
       "      <td>13117.197270</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.889418</td>\n",
       "      <td>1.005371</td>\n",
       "      <td>0.056802</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>19.475906</td>\n",
       "      <td>41.280720</td>\n",
       "      <td>13117.197270</td>\n",
       "      <td>1902.962891</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T23:42:07.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-generic_user:371</td>\n",
       "      <td>2023-04-29T00:49:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>662</td>\n",
       "      <td>508</td>\n",
       "      <td>17.724892</td>\n",
       "      <td>18548.281250</td>\n",
       "      <td>13169.412110</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.893549</td>\n",
       "      <td>1.014733</td>\n",
       "      <td>0.062582</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>19.514097</td>\n",
       "      <td>41.364834</td>\n",
       "      <td>13169.412110</td>\n",
       "      <td>1910.482056</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T23:42:19.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-generic_user:371</td>\n",
       "      <td>2023-04-29T00:49:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>663</td>\n",
       "      <td>509</td>\n",
       "      <td>17.752019</td>\n",
       "      <td>18621.974610</td>\n",
       "      <td>13221.737300</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.897681</td>\n",
       "      <td>1.024139</td>\n",
       "      <td>0.068390</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>19.552292</td>\n",
       "      <td>41.448956</td>\n",
       "      <td>13221.737300</td>\n",
       "      <td>1918.017578</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T23:50:20.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-generic_user:371</td>\n",
       "      <td>2023-04-29T00:49:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>664</td>\n",
       "      <td>510</td>\n",
       "      <td>17.779143</td>\n",
       "      <td>18695.804690</td>\n",
       "      <td>13274.160160</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.901813</td>\n",
       "      <td>1.033589</td>\n",
       "      <td>0.074226</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>19.590479</td>\n",
       "      <td>41.533058</td>\n",
       "      <td>13274.160160</td>\n",
       "      <td>1925.567383</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T23:55:22.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-generic_user:371</td>\n",
       "      <td>2023-04-29T00:49:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>665</td>\n",
       "      <td>511</td>\n",
       "      <td>17.806277</td>\n",
       "      <td>18769.781250</td>\n",
       "      <td>13326.686520</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.905944</td>\n",
       "      <td>1.043079</td>\n",
       "      <td>0.080086</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>valid_passcode</td>\n",
       "      <td>19.628668</td>\n",
       "      <td>41.617168</td>\n",
       "      <td>13326.686520</td>\n",
       "      <td>1933.132324</td>\n",
       "      <td>z</td>\n",
       "      <td>2022-08-31T23:56:43.000000000Z</td>\n",
       "      <td>attacktarget</td>\n",
       "      <td>DFP-duo-generic_user:371</td>\n",
       "      <td>2023-04-29T00:49:14Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  logcount  logcount_pred  logcount_loss  logcount_z_loss  \\\n",
       "0            15         0       5.698269       1.879187         0.662442   \n",
       "1            29        14       5.711531       3.975875         2.250193   \n",
       "2            30        15       5.698916       5.006695         3.030799   \n",
       "3            31        16       5.684619       6.158211         3.902803   \n",
       "4            32        17       5.670151       7.429033         4.865154   \n",
       "..          ...       ...            ...            ...              ...   \n",
       "994         661       507      17.697769   18474.744140     13117.197270   \n",
       "995         662       508      17.724892   18548.281250     13169.412110   \n",
       "996         663       509      17.752019   18621.974610     13221.737300   \n",
       "997         664       510      17.779143   18695.804690     13274.160160   \n",
       "998         665       511      17.806277   18769.781250     13326.686520   \n",
       "\n",
       "     locincrement  locincrement_pred  locincrement_loss  locincrement_z_loss  \\\n",
       "0             1.0           1.991324           2.577614             1.317140   \n",
       "1             1.0           1.972813           2.482248             1.239232   \n",
       "2             1.0           1.970755           2.471756             1.230660   \n",
       "3             1.0           1.968191           2.458717             1.220008   \n",
       "4             1.0           1.966020           2.447701             1.211009   \n",
       "..            ...                ...                ...                  ...   \n",
       "994           3.0           3.889418           1.005371             0.056802   \n",
       "995           3.0           3.893549           1.014733             0.062582   \n",
       "996           3.0           3.897681           1.024139             0.068390   \n",
       "997           3.0           3.901813           1.033589             0.074226   \n",
       "998           3.0           3.905944           1.043079             0.080086   \n",
       "\n",
       "     result  ...     reason_pred reason_loss  reason_z_loss     max_abs_z  \\\n",
       "0     False  ...  valid_passcode    1.165407       4.262609      4.262609   \n",
       "1     False  ...  valid_passcode    1.126028       3.074252      3.976172   \n",
       "2     False  ...  valid_passcode    1.124552       3.029720      4.107398   \n",
       "3     False  ...  valid_passcode    1.124198       3.019036      4.208632   \n",
       "4     False  ...  valid_passcode    1.124767       3.036199      4.865154   \n",
       "..      ...  ...             ...         ...            ...           ...   \n",
       "994   False  ...  valid_passcode   19.475906      41.280720  13117.197270   \n",
       "995   False  ...  valid_passcode   19.514097      41.364834  13169.412110   \n",
       "996   False  ...  valid_passcode   19.552292      41.448956  13221.737300   \n",
       "997   False  ...  valid_passcode   19.590479      41.533058  13274.160160   \n",
       "998   False  ...  valid_passcode   19.628668      41.617168  13326.686520   \n",
       "\n",
       "      mean_abs_z  z_loss_scaler_type                       timestamp  \\\n",
       "0       2.076225                   z  2022-08-31T00:01:31.000000000Z   \n",
       "1       2.126213                   z  2022-08-31T00:40:41.000000000Z   \n",
       "2       2.296612                   z  2022-08-31T00:48:22.000000000Z   \n",
       "3       2.498048                   z  2022-08-31T00:53:18.000000000Z   \n",
       "4       2.720832                   z  2022-08-31T00:54:48.000000000Z   \n",
       "..           ...                 ...                             ...   \n",
       "994  1902.962891                   z  2022-08-31T23:42:07.000000000Z   \n",
       "995  1910.482056                   z  2022-08-31T23:42:19.000000000Z   \n",
       "996  1918.017578                   z  2022-08-31T23:50:20.000000000Z   \n",
       "997  1925.567383                   z  2022-08-31T23:55:22.000000000Z   \n",
       "998  1933.132324                   z  2022-08-31T23:56:43.000000000Z   \n",
       "\n",
       "         username             model_version            event_time  \n",
       "0    attacktarget  DFP-duo-attacktarget:191  2023-04-29T00:49:13Z  \n",
       "1    attacktarget  DFP-duo-attacktarget:191  2023-04-29T00:49:13Z  \n",
       "2    attacktarget  DFP-duo-attacktarget:191  2023-04-29T00:49:13Z  \n",
       "3    attacktarget  DFP-duo-attacktarget:191  2023-04-29T00:49:13Z  \n",
       "4    attacktarget  DFP-duo-attacktarget:191  2023-04-29T00:49:13Z  \n",
       "..            ...                       ...                   ...  \n",
       "994  attacktarget  DFP-duo-generic_user:371  2023-04-29T00:49:14Z  \n",
       "995  attacktarget  DFP-duo-generic_user:371  2023-04-29T00:49:14Z  \n",
       "996  attacktarget  DFP-duo-generic_user:371  2023-04-29T00:49:14Z  \n",
       "997  attacktarget  DFP-duo-generic_user:371  2023-04-29T00:49:14Z  \n",
       "998  attacktarget  DFP-duo-generic_user:371  2023-04-29T00:49:14Z  \n",
       "\n",
       "[999 rows x 36 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = cudf.read_csv(\"dfp_detections_duo.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
