{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - Azure Integrated Training\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs both training and inference on Azure-AD logs. The goal is to train an autoencoder PyTorch model to recogize the patterns of users in the sample data. The model will then be used by another fork (inference) in the pipeline to generate anomaly scores for each individual log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP integrated training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import typing\n",
    "import cudf\n",
    "from datetime import datetime\n",
    "\n",
    "# When segment modules are imported, they're added to the module registry.\n",
    "# To avoid flake8 warnings about unused code, the noqa flag is used during import.\n",
    "import morpheus_dfp.modules  # noqa: F401\n",
    "from morpheus_dfp.utils.config_generator import ConfigGenerator\n",
    "from morpheus_dfp.utils.config_generator import generate_ae_config\n",
    "from morpheus_dfp.utils.dfp_arg_parser import DFPArgParser\n",
    "from morpheus_dfp.utils.schema_utils import Schema\n",
    "from morpheus_dfp.utils.schema_utils import SchemaBuilder\n",
    "\n",
    "import morpheus.loaders  # noqa: F401\n",
    "import morpheus.modules  # noqa: F401\n",
    "from morpheus.config import Config\n",
    "from morpheus.pipeline.pipeline import Pipeline\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.stages.general.multi_port_modules_stage import MultiPortModulesStage\n",
    "from morpheus.stages.input.control_message_file_source_stage import ControlMessageFileSourceStage\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The pipeline's functionality can be significantly altered by the following options, which are utilized across the entire pipeline. However, module-specific options also exist. The matching Python script for this notebook, `dfp_integrated_training_batch_pipeline.py`, configures these options through command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name                   | Type                                       | Description                                                                                                                                                                                                                                                                            | Default Value |\n",
    "|------------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `source`            | One of `[\"duo\", \"azure\"]`           | Indicates what type of logs are going to be used in the workload.                                                                                                                                                                                                            | -             |\n",
    "| `train_users`    | One of `[\"all\", \"generic\", \"individual\"]` | Indicates whether or not to train per user or a generic model for all users. Selecting none runs the inference pipeline.                                                                                                                                                                    | -             |\n",
    "| `skip_user`        | List of strings                                       | User IDs to skip. Mutually exclusive with `only_user`.                                                                                                                                                                                                                                               | -             |\n",
    "| `only_user`        | List of strings                                       | Only users specified by this option will be included. Mutually exclusive with `skip_user`.                                                                                                                                                                                                 | -             |\n",
    "| `start_time`       | `str`                                         | The start of the time window, if undefined start_date will be `now()-duration`.                                                                                                                                                                                                  | -             |\n",
    "| `duration`         | `str`                                         | The training duration to run starting from `start_time`.                                                                                                                                                                                                                                              | -             |\n",
    "| `cache_dir`        | `str`                                         | The location to cache data such as S3 downloads and pre-processed data.                                                                                                                                                                                                                    | -             |\n",
    "| `log_level`        | `str`                                         | Specify the logging level to use.                                                                                                                                                                                                                                                                    | `info`        |\n",
    "| `sample_rate_s`    | `int`                                         | Minimum time step, in milliseconds, between object logs.                                                                                                                                                                                                                                           | `0`        |\n",
    "| `silence_monitors`    | `bool`                                         | Controls whether monitors will be verbose logs.                                                                                                                                                                                                                                           | `False`        |\n",
    "| `tracking_uri`     | `str`                                         | The MLflow tracking URI to connect to the tracking backend.                                                                                                                                                                                                                                    | -             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "source = \"azure\"\n",
    "\n",
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Start time\n",
    "start_time = datetime.strptime(\"2022-08-01\", \"%Y-%m-%d\")\n",
    "\n",
    "# Duration\n",
    "duration = \"60d\"\n",
    "\n",
    "# Smaple rate secs\n",
    "sample_rate_s = 0\n",
    "\n",
    "# MLFLow tracking uri\n",
    "tracking_uri = \"http://mlflow:5000\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_user: typing.List[str] = []\n",
    "\n",
    "# Only users\n",
    "only_user: typing.List[str] = []\n",
    "\n",
    "# Setting Log level\n",
    "log_level = logging.INFO\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"./.cache/dfp\"\n",
    "\n",
    "# Silence monitors\n",
    "silence_monitors = True\n",
    "\n",
    "# Control messages as input files\n",
    "load_train_only_input_files = [\n",
    "    \"./resource/azure_payload_lt.json\"\n",
    "]\n",
    "load_train_inference_input_files = [\n",
    "    \"./resource/azure_payload_lti.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f80a19",
   "metadata": {},
   "source": [
    "### Arguments Parser\n",
    "\n",
    "The [DFPArgParser](../../../production/morpheus/dfp/utils/dfp_arg_parser.py) class is used for parsing and storing arguments used in a  pipeline for training, generating models and inference. It has several properties and methods to transform, store and access the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_arg_parser = DFPArgParser(\n",
    "    skip_user,\n",
    "    only_user,\n",
    "    start_time,\n",
    "    log_level,\n",
    "    cache_dir,\n",
    "    sample_rate_s,\n",
    "    duration,\n",
    "    source,\n",
    "    tracking_uri,\n",
    "    silence_monitors,\n",
    "    train_users\n",
    ")\n",
    "\n",
    "# Initalize parser\n",
    "dfp_arg_parser.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global config object for the pipeline\n",
    "config: Config = generate_ae_config(\n",
    "    source,\n",
    "    userid_column_name=\"username\",\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    use_cpp=True,\n",
    ")\n",
    "\n",
    "# Construct the dataframe Schema which is used to normalize incoming azure logs\n",
    "schema_builder = SchemaBuilder(config, source)\n",
    "schema: Schema = schema_builder.build_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb42888",
   "metadata": {},
   "source": [
    "### DFP Deployment Module Configuration\n",
    "This module sets up modular Digital Fingerprinting intergated training pipeline instance.\n",
    "\n",
    "### Configurable Parameters\n",
    "\n",
    "| Parameter           | Type | Description                               | Example Value | Default Value |\n",
    "|---------------------|------|-------------------------------------------|---------------|---------------|\n",
    "| `inference_options` | dict | Options for the inference pipeline module | See Below     | `[Required]`  |\n",
    "| `training_options`  | dict | Options for the training pipeline module  | See Below     | `[Required]`  |\n",
    "\n",
    "### Training Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|---------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`           |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`    |\n",
    "| `dfencoder_options`          | dict | Options for configuring the data frame encoder | See Below            | `-`           |\n",
    "| `mlflow_writer_options`      | dict | Options for the MLflow model writer            | See Below            | `-`           |\n",
    "| `preprocessing_options`      | dict | Options for preprocessing the data             | See Below            | `-`           |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`           |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column used in the data  | \"my_timestamp\"       | `timestamp`   |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`           |\n",
    "\n",
    "### Inference Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value  |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|----------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`            |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`     |\n",
    "| `detection_criteria`         | dict | Criteria for filtering detections              | See Below            | `-`            |\n",
    "| `fallback_username`          | str  | User ID to use if user ID not found            | \"generic_user\"       | `generic_user` |\n",
    "| `inference_options`          | dict | Options for the inference module               | See Below            | `-`            |\n",
    "| `model_name_formatter`       | str  | Format string for the model name               | \"model_{timestamp}\"  | `[Required]`   |\n",
    "| `num_output_ports`           | int  | Number of output ports for the module          | 3                    | `-`            |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column in the input data | \"timestamp\"          | `timestamp`    |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`            |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`            |\n",
    "| `write_to_file_options`      | dict | Options for writing the detections to a file   | See Below            | `-`            |\n",
    "\n",
    "### `batching_options`\n",
    "\n",
    "| Key                      | Type            | Description                         | Example Value                               | Default Value              |\n",
    "|--------------------------|-----------------|-------------------------------------|---------------------------------------------|----------------------------|\n",
    "| `end_time`               | datetime/string | Endtime of the time window          | \"2023-03-14T23:59:59\"                       | `None`                     |\n",
    "| `iso_date_regex_pattern` | string          | Regex pattern for ISO date matching | \"\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\" | `<iso_date_regex_pattern>` |\n",
    "| `parser_kwargs`          | dictionary      | Additional arguments for the parser | {}                                          | `{}`                       |\n",
    "| `period`                 | string          | Time period for grouping files      | \"1d\"                                        | `D`                        |\n",
    "| `sampling_rate_s`        | integer         | Sampling rate in seconds            | 0                                          | `None`                       |\n",
    "| `start_time`             | datetime/string | Start time of the time window       | \"2023-03-01T00:00:00\"                       | `None`                     |\n",
    "\n",
    "### `dfencoder_options`\n",
    "\n",
    "| Parameter         | Type  | Description                            | Example Value                                                                                                                                                                                                                                                 | Default Value |\n",
    "|-------------------|-------|----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `feature_columns` | list  | List of feature columns to train on    | [\"column1\", \"column2\", \"column3\"]                                                                                                                                                                                                                             | `-`           |\n",
    "| `epochs`          | int   | Number of epochs to train for          | 50                                                                                                                                                                                                                                                            | `-`           |\n",
    "| `model_kwargs`    | dict  | Keyword arguments to pass to the model | {\"encoder_layers\": [64, 32], \"decoder_layers\": [32, 64], \"activation\": \"relu\", \"swap_p\": 0.1, \"lr\": 0.001, \"lr_decay\": 0.9, \"batch_size\": 32, \"verbose\": 1, \"optimizer\": \"adam\", \"scalar\": \"min_max\", \"min_cats\": 10, \"progress_bar\": false, \"device\": \"cpu\"} | `-`           |\n",
    "| `validation_size` | float | Size of the validation set             | 0.1                                                                                                                                                                                                                                                           | `-`           |\n",
    "\n",
    "### `monitor_options`\n",
    "\n",
    "| Key                          | Type    | Description                                                | Example Value | Default Value |\n",
    "| ----------------------------|---------|------------------------------------------------------------|---------------|---------------|\n",
    "| `description`               | string  | Name to show for this Monitor Stage in the console window  | \"Progress\"    | `Progress`    |\n",
    "| `silence_monitors`          | bool    | Silence the monitors on the console                        | See Below     | `None`        |\n",
    "| `smoothing`                 | float   | Smoothing parameter to determine how much the throughput should be averaged | 0.01 | `0.05` |\n",
    "| `unit`                      | string  | Units to show in the rate value                             | \"messages\"    | `messages`    |\n",
    "| `delayed_start`             | bool    | When delayed_start is enabled, the progress bar will not be shown until the first message is received. Otherwise, the progress bar is shown on pipeline startup and will begin timing immediately. In large pipelines, this option may be desired to give a more accurate timing. | True  | `False`   |\n",
    "| `determine_count_fn_schema` | string  | Custom function for determining the count in a message      | \"Progress\"    | `Progress`    |\n",
    "| `log_level`                 | string  | Enable this stage when the configured log level is at `log_level` or lower. | \"DEBUG\" | `INFO` |\n",
    "\n",
    "\n",
    "### `mlflow_writer_options`\n",
    "\n",
    "| Key                         | Type       | Description                       | Example Value                 | Default Value |\n",
    "|-----------------------------|------------|-----------------------------------|-------------------------------|---------------|\n",
    "| `conda_env`                 | string     | Conda environment for the model   | \"path/to/conda_env.yml\"       | `[Required]`  |\n",
    "| `databricks_permissions`    | dictionary | Permissions for the model         | See Below                     | `None`        |\n",
    "| `experiment_name_formatter` | string     | Formatter for the experiment name | \"experiment_name_{timestamp}\" | `[Required]`  |\n",
    "| `model_name_formatter`      | string     | Formatter for the model name      | \"model_name_{timestamp}\"      | `[Required]`  |\n",
    "| `timestamp_column_name`     | string     | Name of the timestamp column      | \"timestamp\"                   | `timestamp`   |\n",
    "\n",
    "### `stream_aggregation_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                                 | Example Value | Default Value |\n",
    "|-------------------------|--------|-------------------------------------------------------------|---------------|---------------|\n",
    "| `cache_mode`            | string | Mode for managing user cache. Setting to `batch` flushes cache once trigger conditions are met. Otherwise, continue to aggregate user's history.              | \"batch\"       | `batch`       |\n",
    "| `min_history`           | int    | Minimum history to trigger a new training event             | 1             | `1`           |\n",
    "| `max_history`           | int    | Maximum history to include in a new training event          | 0             | `0`           |\n",
    "| `timestamp_column_name` | string | Name of the column containing timestamps                    | \"timestamp\"   | `timestamp`   |\n",
    "| `aggregation_span`      | string | Lookback timespan for training data in a new training event | \"60d\"         | `60d`         |\n",
    "| `cache_to_disk`         | bool   | Whether or not to cache streaming data to disk              | false         | `false`       |\n",
    "| `cache_dir`             | string | Directory to use for caching streaming data                 | \"./.cache\"    | `./.cache`    |\n",
    "\n",
    "### `user_splitting_options`\n",
    "\n",
    "| Key                     | Type | Description                                          | Example Value               | Default Value  |\n",
    "|-------------------------|------|------------------------------------------------------|-----------------------------|----------------|\n",
    "| `fallback_username`     | str  | The user ID to use if the user ID is not found       | \"generic_user\"              | `generic_user` |\n",
    "| `include_generic`       | bool | Whether to include a generic user ID in the output   | false                       | `false`        |\n",
    "| `include_individual`    | bool | Whether to include individual user IDs in the output | true                        | `false`        |\n",
    "| `only_users`            | list | List of user IDs to include; others will be excluded | [\"user1\", \"user2\", \"user3\"] | `[]`           |\n",
    "| `skip_users`            | list | List of user IDs to exclude from the output          | [\"user4\", \"user5\"]          | `[]`           |\n",
    "| `timestamp_column_name` | str  | Name of the column containing timestamps             | \"timestamp\"                 | `timestamp`    |\n",
    "| `userid_column_name`    | str  | Name of the column containing user IDs               | \"username\"                  | `username`     |\n",
    "\n",
    "### `detection_criteria`\n",
    "\n",
    "| Key          | Type  | Description                              | Example Value | Default Value |\n",
    "|--------------|-------|------------------------------------------|---------------|---------------|\n",
    "| `threshold`  | float | Threshold for filtering detections       | 0.5           | `0.5`         |\n",
    "| `field_name` | str   | Name of the field to filter by threshold | \"score\"       | `probs`       |\n",
    "\n",
    "### `inference_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                          | Example Value           | Default Value |\n",
    "|-------------------------|--------|------------------------------------------------------|-------------------------|---------------|\n",
    "| `model_name_formatter`  | string | Formatter for model names                            | \"user_{username}_model\" | `[Required]`  |\n",
    "| `fallback_username`     | string | Fallback user to use if no model is found for a user | \"generic_user\"          | `generic_user`|\n",
    "| `timestamp_column_name` | string | Name of the timestamp column                         | \"timestamp\"             | `timestamp`   |\n",
    "\n",
    "### `write_to_file_options`\n",
    "\n",
    "| Key                 | Type      | Description                              | Example Value   | Default Value    |\n",
    "|---------------------|-----------|------------------------------------------|-----------------|------------------|\n",
    "| `filename`          | string    | Path to the output file                  | \"output.csv\"    | `None`           |\n",
    "| `file_type`         | string    | Type of file to write                    | \"CSV\"           | `AUTO`           |\n",
    "| `flush`             | bool      | If true, flush the file after each write | false           | `false`          |\n",
    "| `include_index_col` | bool      | If true, include the index column        | false           | `true`           |\n",
    "| `overwrite`         | bool      | If true, overwrite the file if it exists | true            | `false`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config helper is used to generate config parameters for the DFP module\n",
    "# This will populate to the minimum configuration parameters with intelligent default values\n",
    "config_generator = ConfigGenerator(config, dfp_arg_parser, schema)\n",
    "\n",
    "dfp_deployment_module_config = config_generator.get_module_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`ControlMessageFileSourceStage`)\n",
    "\n",
    "This pipeline read control message definations from one or more input files. This source stage will constructs control message and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any control message defination files to read into the pipeline |\n",
    "\n",
    "### DFP Deployment Module (`MultiPortModulesStage`)\n",
    "\n",
    "MultiPortModulesStage is used to load modules that returns more than one ouptut. DFP deployment module sets up modular Digital Fingerprinting Pipeline instance. and performs integrated training as shown in the below diagram. For more information on the options passed to this module is shown [here](../../../../../docs/source/modules/examples/digital_fingerprinting/dfp_deployment.md).\n",
    "\n",
    "\n",
    "## End-to-End Workflow Architecture\n",
    "\n",
    "![Integrated Training Pipeline](../../../../../docs/source/img/dfp_integrated_training_file_pipeline.png))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pipeline(input_files: list[str]):\n",
    "    # Create a pipeline object\n",
    "    pipeline = Pipeline(config)\n",
    "\n",
    "    # ControlMessage file source stage.\n",
    "    source_stage = pipeline.add_stage(ControlMessageFileSourceStage(config, filenames=input_files))\n",
    "\n",
    "    # DFP deployment (integrated training) module stage.\n",
    "    dfp_deployment_stage = pipeline.add_stage(\n",
    "        MultiPortModulesStage(config,\n",
    "                                dfp_deployment_module_config,\n",
    "                                input_ports=[\"input\"],\n",
    "                                output_ports=[\"output_0\", \"output_1\"]))\n",
    "\n",
    "    # Connect stages with edges.\n",
    "    pipeline.add_edge(source_stage, dfp_deployment_stage)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b93cb",
   "metadata": {},
   "source": [
    "### Training\n",
    "To ensure a smooth deployment of the inference tasks to the pipeline, it is imperative to have at least one version of the trained model available on the MLflow server. Once the initial model training is complete, we can proceed with the publishing of the inference tasks to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_only_input_files\n",
    "\n",
    "pipeline = construct_pipeline(input_files)\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1472a0",
   "metadata": {},
   "source": [
    "### Training and Inference\n",
    "Now that we have a trained model available in MLflow, we can begin executing both training and inference tasks in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64339676",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = load_train_inference_input_files\n",
    "\n",
    "pipeline = construct_pipeline(input_files)\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24caa6f",
   "metadata": {},
   "source": [
    "### Inference Results\n",
    "Pipeline writes the inference results to `dfp_detections_azure.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f14ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"dfp_detections_azure.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689655f0-51ce-43a7-81bf-5e9ee2521911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:morpheus] *",
   "language": "python",
   "name": "conda-env-morpheus-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
