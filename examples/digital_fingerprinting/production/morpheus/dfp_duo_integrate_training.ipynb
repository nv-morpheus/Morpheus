{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2941e94f-db20-44a5-ab87-2cab499825f7",
   "metadata": {},
   "source": [
    "# Digital Finger Printing (DFP) with Morpheus - DUO Integrated Training\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be building and running a DFP pipeline that performs both training and inference on Duo authentication logs. The goal is to train an autoencoder PyTorch model to recogize the patterns of users in the sample data. The model will then be used by another fork (inference) in the pipeline to generate anomaly scores for each individual log. These anomaly scores can be used by security teams to detect abnormal behavior when it happens so the proper action can be taken.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For more information on DFP, the Morpheus pipeline, and setup steps to run this notebook, please refer to the coresponding DFP integrated training materials.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1cb50-74f2-445d-b865-8c22c3b3798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Ensure that the morpheus directory is in the python path. This may not need to be run depending on the environment setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"./morpheus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ce011-3ca3-4f96-a72d-de28fad32003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>table {align:left;display:block}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import typing\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import dfp.modules.dfp_deployment  # noqa: F401\n",
    "from dfp.utils.config_generator import ConfigGenerator\n",
    "from dfp.utils.config_generator import generate_ae_config\n",
    "from dfp.utils.dfp_arg_parser import DFPArgParser\n",
    "from dfp.utils.schema_utils import Schema\n",
    "from dfp.utils.schema_utils import SchemaBuilder\n",
    "\n",
    "from morpheus.cli.utils import get_log_levels\n",
    "from morpheus.cli.utils import parse_log_level\n",
    "from morpheus.config import Config\n",
    "from morpheus.pipeline.pipeline import Pipeline\n",
    "from morpheus.stages.general.monitor_stage import MonitorStage\n",
    "from morpheus.stages.general.multiport_modules_stage import MultiPortModulesStage\n",
    "from morpheus.stages.input.control_message_kafka_source_stage import ControlMessageKafkaSourceStage\n",
    "\n",
    "# Left align all tables\n",
    "from IPython.core.display import HTML\n",
    "table_css = 'table {align:left;display:block}'\n",
    "HTML('<style>{}</style>'.format(table_css))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca38c1b7-ce84-43e0-ac53-280562dc1642",
   "metadata": {},
   "source": [
    "## High Level Configuration\n",
    "\n",
    "The pipeline's functionality can be significantly altered by the following options, which are utilized across the entire pipeline. However, module-specific options also exist. The matching Python script for this notebook, `dfp_integrated_training_batch_pipeline.py`, configures these options through command line arguments.\n",
    "\n",
    "### Options\n",
    "\n",
    "| Name                   | Type                                       | Description                                                                                                                                                                                                                                                                            | Default Value |\n",
    "|------------------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `source`            | One of `[\"duo\", \"azure\"]`           | Indicates what type of logs are going to be used in the workload.                                                                                                                                                                                                            | -             |\n",
    "| `train_users`    | One of `[\"all\", \"generic\", \"individual\"]` | Indicates whether or not to train per user or a generic model for all users. Selecting none runs the inference pipeline.                                                                                                                                                                    | -             |\n",
    "| `skip_user`        | List of strings                                       | User IDs to skip. Mutually exclusive with `only_user`.                                                                                                                                                                                                                                               | -             |\n",
    "| `only_user`        | List of strings                                       | Only users specified by this option will be included. Mutually exclusive with `skip_user`.                                                                                                                                                                                                 | -             |\n",
    "| `start_time`       | `str`                                         | The start of the time window, if undefined start_date will be `now()-duration`.                                                                                                                                                                                                  | -             |\n",
    "| `duration`         | `str`                                         | The training duration to run starting from `start_time`.                                                                                                                                                                                                                                              | -             |\n",
    "| `use_cpp`          | `bool`                                       | Indicates whether or not to use C++ extensions for optimization.                                                                                                                                                                                                                                   | `false`       |\n",
    "| `cache_dir`        | `str`                                         | The location to cache data such as S3 downloads and pre-processed data.                                                                                                                                                                                                                    | -             |\n",
    "| `log_level`        | `str`                                         | Specify the logging level to use.                                                                                                                                                                                                                                                                    | `info`        |\n",
    "| `sample_rate_s`    | `int`                                         | Minimum time step, in milliseconds, between object logs.                                                                                                                                                                                                                                           | `0`        |\n",
    "| `tracking_uri`     | `str`                                         | The MLflow tracking URI to connect to the tracking backend.                                                                                                                                                                                                                                    | -             |\n",
    "| `disable_pre_filtering` | `bool`                                | Enabling this option will skip pre-filtering of JSON messages. This is only useful when inputs are known to be valid JSON.                                                                                                                                                                                                                                     | `false`       |\n",
    "| `input_file`        | `str`                                         | List of control message definition files to process. Can specify multiple arguments for multiple files. Also accepts glob (*) wildcards. Refer to `fsspec` documentation for list of possible options.                                                                                                                                             | -             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee00703-75c5-46fc-890c-86733da906c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source\n",
    "source = \"duo\"\n",
    "\n",
    "# Global options\n",
    "train_users = \"all\"\n",
    "\n",
    "# Start time\n",
    "start_time = \"2022-08-01\"\n",
    "\n",
    "# Duration\n",
    "duration = \"60d\"\n",
    "\n",
    "# Smaple rate secs\n",
    "sample_rate_s = 0\n",
    "\n",
    "# MLFLow tracking uri\n",
    "tracking_uri = \"http://mlflow:5000\"\n",
    "\n",
    "# Enter any users to skip here\n",
    "skip_users: typing.List[str] = []\n",
    "\n",
    "# \n",
    "only_users: typing.List[str] = []\n",
    "\n",
    "# Location where cache objects will be saved\n",
    "cache_dir = \"/workspace/.cache/dfp\"\n",
    "\n",
    "# Input files to read from\n",
    "input_files = [\n",
    "    \"/workspace/examples/data/dfp/duo-training-data/DUO_2022-08-*.json\"\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f80a19",
   "metadata": {},
   "source": [
    "### Arguments Parser\n",
    "\n",
    "The [DFPArgParser](../../production/morpheus/dfp/utils/dfp_arg_parser.py) class is used for parsing and storing arguments used in a  pipeline for training, generating models and inference. It has several properties and methods to transform, store and access the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp_arg_parser = DFPArgParser(\n",
    "    skip_user,\n",
    "    only_user,\n",
    "    start_time,\n",
    "    log_level,\n",
    "    cache_dir,\n",
    "    sample_rate_s,\n",
    "    duration,\n",
    "    source,\n",
    "    tracking_uri,\n",
    "    train_users\n",
    ")\n",
    "\n",
    "# Initalize parser\n",
    "dfp_arg_parser.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66f6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd537-9162-49dc-8e83-d9465592f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global config object for the pipeline\n",
    "config: Config = generate_ae_config(\n",
    "    source,\n",
    "    userid_column_name=\"username\",\n",
    "    timestamp_column_name=\"timestamp\",\n",
    "    use_cpp=use_cpp,\n",
    ")\n",
    "\n",
    "# Construct the dataframe Schema used to normalize incoming duo logs\n",
    "schema_builder = SchemaBuilder(config, source)\n",
    "schema: Schema = schema_builder.build_schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdb42888",
   "metadata": {},
   "source": [
    "### DFP Deployment Module Configuration\n",
    "This module function sets up modular Digital Fingerprinting Pipeline instance.\n",
    "\n",
    "### Configurable Parameters\n",
    "\n",
    "| Parameter           | Type | Description                               | Example Value | Default Value |\n",
    "|---------------------|------|-------------------------------------------|---------------|---------------|\n",
    "| `inference_options` | dict | Options for the inference pipeline module | See Below     | `[Required]`  |\n",
    "| `training_options`  | dict | Options for the training pipeline module  | See Below     | `[Required]`  |\n",
    "\n",
    "### Training Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|---------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`           |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`    |\n",
    "| `dfencoder_options`          | dict | Options for configuring the data frame encoder | See Below            | `-`           |\n",
    "| `mlflow_writer_options`      | dict | Options for the MLflow model writer            | See Below            | `-`           |\n",
    "| `preprocessing_options`      | dict | Options for preprocessing the data             | See Below            | `-`           |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`           |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column used in the data  | \"my_timestamp\"       | `timestamp`   |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`           |\n",
    "\n",
    "### Inference Options Parameters\n",
    "\n",
    "| Parameter                    | Type | Description                                    | Example Value        | Default Value  |\n",
    "|------------------------------|------|------------------------------------------------|----------------------|----------------|\n",
    "| `batching_options`           | dict | Options for batching the data                  | See Below            | `-`            |\n",
    "| `cache_dir`                  | str  | Directory to cache the rolling window data     | \"/path/to/cache/dir\" | `./.cache`     |\n",
    "| `detection_criteria`         | dict | Criteria for filtering detections              | See Below            | `-`            |\n",
    "| `fallback_username`          | str  | User ID to use if user ID not found            | \"generic_user\"       | `generic_user` |\n",
    "| `inference_options`          | dict | Options for the inference module               | See Below            | `-`            |\n",
    "| `model_name_formatter`       | str  | Format string for the model name               | \"model_{timestamp}\"  | `[Required]`   |\n",
    "| `num_output_ports`           | int  | Number of output ports for the module          | 3                    | `-`            |\n",
    "| `timestamp_column_name`      | str  | Name of the timestamp column in the input data | \"timestamp\"          | `timestamp`    |\n",
    "| `stream_aggregation_options` | dict | Options for aggregating the data by stream     | See Below            | `-`            |\n",
    "| `user_splitting_options`     | dict | Options for splitting the data by user         | See Below            | `-`            |\n",
    "| `write_to_file_options`      | dict | Options for writing the detections to a file   | See Below            | `-`            |\n",
    "\n",
    "### `batching_options`\n",
    "\n",
    "| Key                      | Type            | Description                         | Example Value                               | Default Value              |\n",
    "|--------------------------|-----------------|-------------------------------------|---------------------------------------------|----------------------------|\n",
    "| `end_time`               | datetime/string | Endtime of the time window          | \"2023-03-14T23:59:59\"                       | `None`                     |\n",
    "| `iso_date_regex_pattern` | string          | Regex pattern for ISO date matching | \"\\\\d{4}-\\\\d{2}-\\\\d{2}T\\\\d{2}:\\\\d{2}:\\\\d{2}\" | `<iso_date_regex_pattern>` |\n",
    "| `parser_kwargs`          | dictionary      | Additional arguments for the parser | {}                                          | `{}`                       |\n",
    "| `period`                 | string          | Time period for grouping files      | \"1d\"                                        | `D`                        |\n",
    "| `sampling_rate_s`        | integer         | Sampling rate in seconds            | 60                                          | `60`                       |\n",
    "| `start_time`             | datetime/string | Start time of the time window       | \"2023-03-01T00:00:00\"                       | `None`                     |\n",
    "\n",
    "### `dfencoder_options`\n",
    "\n",
    "| Parameter         | Type  | Description                            | Example Value                                                                                                                                                                                                                                                 | Default Value |\n",
    "|-------------------|-------|----------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
    "| `feature_columns` | list  | List of feature columns to train on    | [\"column1\", \"column2\", \"column3\"]                                                                                                                                                                                                                             | `-`           |\n",
    "| `epochs`          | int   | Number of epochs to train for          | 50                                                                                                                                                                                                                                                            | `-`           |\n",
    "| `model_kwargs`    | dict  | Keyword arguments to pass to the model | {\"encoder_layers\": [64, 32], \"decoder_layers\": [32, 64], \"activation\": \"relu\", \"swap_p\": 0.1, \"lr\": 0.001, \"lr_decay\": 0.9, \"batch_size\": 32, \"verbose\": 1, \"optimizer\": \"adam\", \"scalar\": \"min_max\", \"min_cats\": 10, \"progress_bar\": false, \"device\": \"cpu\"} | `-`           |\n",
    "| `validation_size` | float | Size of the validation set             | 0.1                                                                                                                                                                                                                                                           | `-`           |\n",
    "\n",
    "### `mlflow_writer_options`\n",
    "\n",
    "| Key                         | Type       | Description                       | Example Value                 | Default Value |\n",
    "|-----------------------------|------------|-----------------------------------|-------------------------------|---------------|\n",
    "| `conda_env`                 | string     | Conda environment for the model   | \"path/to/conda_env.yml\"       | `[Required]`  |\n",
    "| `databricks_permissions`    | dictionary | Permissions for the model         | See Below                     | `None`        |\n",
    "| `experiment_name_formatter` | string     | Formatter for the experiment name | \"experiment_name_{timestamp}\" | `[Required]`  |\n",
    "| `model_name_formatter`      | string     | Formatter for the model name      | \"model_name_{timestamp}\"      | `[Required]`  |\n",
    "| `timestamp_column_name`     | string     | Name of the timestamp column      | \"timestamp\"                   | `timestamp`   |\n",
    "\n",
    "### `stream_aggregation_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                                 | Example Value | Default Value |\n",
    "|-------------------------|--------|-------------------------------------------------------------|---------------|---------------|\n",
    "| `cache_mode`            | string | The user ID to use if the user ID is not found              | \"batch\"       | `batch`       |\n",
    "| `min_history`           | int    | Minimum history to trigger a new training event             | 1             | `1`           |\n",
    "| `max_history`           | int    | Maximum history to include in a new training event          | 0             | `0`           |\n",
    "| `timestamp_column_name` | string | Name of the column containing timestamps                    | \"timestamp\"   | `timestamp`   |\n",
    "| `aggregation_span`      | string | Lookback timespan for training data in a new training event | \"60d\"         | `60d`         |\n",
    "| `cache_to_disk`         | bool   | Whether or not to cache streaming data to disk              | false         | `false`       |\n",
    "| `cache_dir`             | string | Directory to use for caching streaming data                 | \"./.cache\"    | `./.cache`    |\n",
    "\n",
    "### `user_splitting_options`\n",
    "\n",
    "| Key                     | Type | Description                                          | Example Value               | Default Value  |\n",
    "|-------------------------|------|------------------------------------------------------|-----------------------------|----------------|\n",
    "| `fallback_username`     | str  | The user ID to use if the user ID is not found       | \"generic_user\"              | `generic_user` |\n",
    "| `include_generic`       | bool | Whether to include a generic user ID in the output   | false                       | `false`        |\n",
    "| `include_individual`    | bool | Whether to include individual user IDs in the output | true                        | `false`        |\n",
    "| `only_users`            | list | List of user IDs to include; others will be excluded | [\"user1\", \"user2\", \"user3\"] | `[]`           |\n",
    "| `skip_users`            | list | List of user IDs to exclude from the output          | [\"user4\", \"user5\"]          | `[]`           |\n",
    "| `timestamp_column_name` | str  | Name of the column containing timestamps             | \"timestamp\"                 | `timestamp`    |\n",
    "| `userid_column_name`    | str  | Name of the column containing user IDs               | \"username\"                  | `username`     |\n",
    "\n",
    "### `detection_criteria`\n",
    "\n",
    "| Key          | Type  | Description                              | Example Value | Default Value |\n",
    "|--------------|-------|------------------------------------------|---------------|---------------|\n",
    "| `threshold`  | float | Threshold for filtering detections       | 0.5           | `0.5`         |\n",
    "| `field_name` | str   | Name of the field to filter by threshold | \"score\"       | `probs`       |\n",
    "\n",
    "### `inference_options`\n",
    "\n",
    "| Parameter               | Type   | Description                                          | Example Value           | Default Value |\n",
    "|-------------------------|--------|------------------------------------------------------|-------------------------|---------------|\n",
    "| `model_name_formatter`  | string | Formatter for model names                            | \"user_{username}_model\" | `[Required]`  |\n",
    "| `fallback_username`     | string | Fallback user to use if no model is found for a user | \"generic_user\"          | `generic_user`|\n",
    "| `timestamp_column_name` | string | Name of the timestamp column                         | \"timestamp\"             | `timestamp`   |\n",
    "\n",
    "### `write_to_file_options`\n",
    "\n",
    "| Key                 | Type      | Description                              | Example Value   | Default Value    |\n",
    "|---------------------|-----------|------------------------------------------|-----------------|------------------|\n",
    "| `filename`          | string    | Path to the output file                  | \"output.csv\"    | `None`           |\n",
    "| `file_type`         | string    | Type of file to write                    | \"CSV\"           | `AUTO`           |\n",
    "| `flush`             | bool      | If true, flush the file after each write | false           | `false`          |\n",
    "| `include_index_col` | bool      | If true, include the index column        | false           | `true`           |\n",
    "| `overwrite`         | bool      | If true, overwrite the file if it exists | true            | `false`          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a4d53-32b6-4ab8-a5d7-c0104b31c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config helper used to generate config parameters for the DFP module\n",
    "# This will populate to the minimum configuration parameters with intelligent default values\n",
    "config_generator = ConfigGenerator(config, dfp_arg_parser, schema)\n",
    "\n",
    "# Generate dfp_deployment_module configuration using config generator\n",
    "dfp_deployment_module_config = config_generator.get_module_conf()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bdfc59de-dea8-4e5f-98e3-98eba1e4621d",
   "metadata": {},
   "source": [
    "## Pipeline Construction\n",
    "From this point on we begin constructing the stages that will make up the pipeline. To make testing easier, constructing the pipeline object, adding the stages, and running the pipeline, is provided as a single cell. The below cell can be rerun multiple times as needed for debugging.\n",
    "\n",
    "### Source Stage (`ControlMessageFileSourceStage`)\n",
    "\n",
    "This pipeline read control message definations from one or more input files. This source stage will constructs control message and pass to downstream stages. It is capable of reading files from many different source types, both local and remote. This is possible by utilizing the `fsspec` library (similar to `pandas`). Refer to the [`fsspec`](https://filesystem-spec.readthedocs.io/) documentation for more information on the supported file types. Once all of the logs have been read, the source completes. \n",
    "\n",
    "| Name | Type | Default | Description |\n",
    "| --- | --- | --- | :-- |\n",
    "| `filenames` | List of strings | | Any control message defination files to read into the pipeline |\n",
    "\n",
    "### DFP Deployment Stage (`MultiPortModulesStage`)\n",
    "\n",
    "This module sets up modular Digital Fingerprinting Pipeline instance. and performs integrated training as shown in the below diagram\n",
    "\n",
    "\n",
    "                                                         +--------------------------------------+ \n",
    "                                                         |             source_stage             |\n",
    "                                                         +--------------------------------------+\n",
    "                                                                            |\n",
    "                                                                            v\n",
    "                                                                +------------------------+\n",
    "                                                                |  dfp_deployment_module |\n",
    "                +---------------------------------------------------------------------------------------------------------------------------+\n",
    "                |                                                           |                                                               |\n",
    "                |                                                           v                                                               |\n",
    "                |                                        +-------------------------------------+                                            |\n",
    "                |                                        |          fsspec_loader_module       |                                            | \n",
    "                |                                        +-------------------------------------+                                            |\n",
    "                |                                                           |                                                               |\n",
    "                |                                                           v                                                               |\n",
    "                |                                        +-------------------------------------+                                            |\n",
    "                |                                        |              broadcast              |                                            | \n",
    "                |                                        +-------------------------------------+                                            |\n",
    "                |                                                   /                \\                                                      |\n",
    "                |                                                  /                  \\                                                     |\n",
    "                |                                                 /                    \\                                                    |\n",
    "                |                                                v                      v                                                   |\n",
    "                |                              +-------------------------+        +-------------------------+                               |\n",
    "                |                              |dfp_trianing_pipe_module |        |dfp_inference_pipe_module|                               |         \n",
    "                |   +----------------------------------------------------+        +-----------------------------------------------------+   |\n",
    "                |   |                                                    |        |                                                     |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |      |           preproc_module            |       |        |       |           preproc_module            |       |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |                        |                           |        |                          |                          |   |\n",
    "                |   |                        v                           |        |                          v                          |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |      |     dfp_rolling_window_module       |       |        |       |      dfp_rolling_window_module      |       |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |                        |                           |        |                          |                          |   |\n",
    "                |   |                        v                           |        |                          v                          |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |      |       dfp_data_prep_module          |       |        |       |         dfp_data_prep_module        |       |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |                        |                           |        |                          |                          |   |\n",
    "                |   |                        v                           |        |                          v                          |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |      |       dfp_training_module           |       |        |       |         dfp_inference_module        |       |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |                        |                           |        |                          |                          |   |\n",
    "                |   |                        v                           |        |                          v                          |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |   |      |     mlflow_model_writer_module      |       |        |       |       filter_detections_module      |       |   |\n",
    "                |   |      +-------------------------------------+       |        |       +-------------------------------------+       |   |\n",
    "                |    ----------------------------------------------------         |                          |                          |   |\n",
    "                |                            |                                    |                          v                          |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                    |       |          dfp_post_proc_module       |       |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                    |                          |                          |   |\n",
    "                |                            |                                    |                          v                          |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                    |       |           serialize_module          |       |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                    |                          |                          |   |\n",
    "                |                            |                                    |                          v                          |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                    |       |         write_to_file_module        |       |   |\n",
    "                |                            |                                    |       +-------------------------------------+       |   |\n",
    "                |                            |                                     -----------------------------------------------------    |\n",
    "                 ----------------------------|---------------------------------------------------------------|------------------------------\n",
    "                                             |                                                               |                     \n",
    "                                             |                                                               |  \n",
    "                                             v                                                               v\n",
    "                           +-------------------------------------+                        +-------------------------------------+       \n",
    "                           |        train_monitor_stage          |                        |         infer_monitor_stage         |\n",
    "                           +-------------------------------------+                        +-------------------------------------+  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825390ad-ce64-4949-b324-33039ffdf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline object\n",
    "pipeline = Pipeline(config)\n",
    "\n",
    "# ControlMessage file source stage.\n",
    "source_stage = pipeline.add_stage(ControlMessageFileSourceStage(config, filenames=filenames))\n",
    "\n",
    "# DFP deployment (integrated training) module stage.\n",
    "dfp_deployment_stage = pipeline.add_stage(\n",
    "    MultiPortModulesStage(config,\n",
    "                            dfp_deployment_module_config,\n",
    "                            input_port_name=\"input\",\n",
    "                            output_port_name_prefix=\"output\",\n",
    "                            num_output_ports=num_output_ports))\n",
    "\n",
    "# Monitor stage for training pipe.\n",
    "train_moniter_stage = pipeline.add_stage(\n",
    "    MonitorStage(config, description=\"DFP Training Pipeline rate\", smoothing=0.001))\n",
    "\n",
    "# Monitor stage for inference pipe.\n",
    "infer_moniter_stage = pipeline.add_stage(\n",
    "    MonitorStage(config, description=\"DFP Inference Pipeline rate\", smoothing=0.001))\n",
    "\n",
    "# Connect stages with edges.\n",
    "pipeline.add_edge(source_stage, dfp_deployment_stage)\n",
    "pipeline.add_edge(dfp_deployment_stage.output_ports[0], train_moniter_stage)\n",
    "pipeline.add_edge(dfp_deployment_stage.output_ports[1], infer_moniter_stage)\n",
    "\n",
    "# Run the pipeline\n",
    "await pipeline.run_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed0657-6f4b-4f21-97fa-051eeb7f4fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
