{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexible log parsing based on the BERT language model\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Generating Labeled Logs\n",
    "* Subword Tokenization\n",
    "* Training and Validation Data\n",
    "* Fine-tuning pretrained BERT\n",
    "* Model Evaluation\n",
    "* Saving Model\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "\n",
    "One of the most arduous tasks of any security operation (and equally as time consuming for a data scientist) is ETL and parsing. This notebook illustrates how to train a BERT language model using a toy dataset of just 1000 previously parsed apache server logs as a labeled data. We will fine-tune a pretrained BERT model from [HuggingFace](https://github.com/huggingface) with a classification layer for Named Entity Recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "from transformers import BertForTokenClassification\n",
    "from tqdm import tqdm,trange\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy\n",
    "import cudf\n",
    "from cudf.core.subword_tokenizer import SubwordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Labels For Our Training Dataset\n",
    "\n",
    "To train our model we begin with a dataframe containing parsed logs and additional `raw` column containing the whole raw log as a string. We will use the column names as our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = cudf.read_csv(\"../../datasets/training-data/log-parsing-training-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error_level</th>\n",
       "      <th>error_message</th>\n",
       "      <th>raw</th>\n",
       "      <th>remote_host</th>\n",
       "      <th>remote_logname</th>\n",
       "      <th>remote_user</th>\n",
       "      <th>request_header_referer</th>\n",
       "      <th>request_header_user_agent</th>\n",
       "      <th>request_header_user_agent__browser__family</th>\n",
       "      <th>request_header_user_agent__os__family</th>\n",
       "      <th>request_header_user_agent__os__version_string</th>\n",
       "      <th>request_http_ver</th>\n",
       "      <th>request_method</th>\n",
       "      <th>request_url</th>\n",
       "      <th>response_bytes_clf</th>\n",
       "      <th>status</th>\n",
       "      <th>time_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>158.69.5.181 - - [04/Apr/2018:23:06:49 +0200] ...</td>\n",
       "      <td>158.69.5.181</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>1.1</td>\n",
       "      <td>POST</td>\n",
       "      <td>/administrator/index.php</td>\n",
       "      <td>4498</td>\n",
       "      <td>200</td>\n",
       "      <td>[04/Apr/2018:23:06:49 +0200]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    error_level error_message  \\\n",
       "257        <NA>          <NA>   \n",
       "\n",
       "                                                   raw   remote_host  \\\n",
       "257  158.69.5.181 - - [04/Apr/2018:23:06:49 +0200] ...  158.69.5.181   \n",
       "\n",
       "    remote_logname remote_user request_header_referer  \\\n",
       "257              -           -                      -   \n",
       "\n",
       "    request_header_user_agent request_header_user_agent__browser__family  \\\n",
       "257                         -                                      Other   \n",
       "\n",
       "    request_header_user_agent__os__family  \\\n",
       "257                                 Other   \n",
       "\n",
       "    request_header_user_agent__os__version_string  request_http_ver  \\\n",
       "257                                          <NA>               1.1   \n",
       "\n",
       "    request_method               request_url response_bytes_clf status  \\\n",
       "257           POST  /administrator/index.php               4498    200   \n",
       "\n",
       "                    time_received  \n",
       "257  [04/Apr/2018:23:06:49 +0200]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample parsed log\n",
    "logs_df.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193.106.31.130 - - [01/Sep/2019:03:28:00 +0200] \"POST /administrator/index.php HTTP/1.0\" 200 4481 \"-\" \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)\" \"-\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample raw log\n",
    "print(logs_df.raw.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text to split on punctuation for bert tokenizer\n",
    "logs_df['raw_preprocess'] = logs_df['raw']\n",
    "for symbol in string.punctuation:\n",
    "    logs_df['raw_preprocess'] = logs_df['raw_preprocess'].str.replace(symbol, ' ' + symbol + ' ')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation_spliter(text):\n",
    "    for symbol in string.punctuation:\n",
    "        text = text.replace(symbol, ' ' + symbol + ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeler(index_no, cols):\n",
    "    \"\"\"\n",
    "    label the words in the raw log with the column name from the parsed log\n",
    "    \"\"\"\n",
    "    raw_split = logs_df.raw_preprocess[index_no].split()\n",
    "    \n",
    "    # words in raw but not in parsed logs labeled as 'other'\n",
    "    label_list = ['O'] * len(raw_split) \n",
    "    \n",
    "    # for each parsed column find the location of the sequence of words (sublist) in the raw log\n",
    "    for col in cols:\n",
    "        col_value = str(logs_df[col][index_no])\n",
    "        if col_value not in {'','-','None','NaN'}:\n",
    "            col_value = punctuation_spliter(col_value)\n",
    "            sublist = col_value.split()\n",
    "            sublist_len=len(sublist)\n",
    "            match_count = 0\n",
    "            for ind in (i for i,el in enumerate(raw_split) if el==sublist[0]):\n",
    "                # words in raw log not present in the parsed log will be labeled with 'O'\n",
    "                if (match_count < 1) and (raw_split[ind:ind+sublist_len]==sublist) and (label_list[ind:ind+sublist_len] == ['O'] * sublist_len):\n",
    "                    label_list[ind] = 'B-'+col\n",
    "                    label_list[ind+1:ind+sublist_len] = ['I-'+col] * (sublist_len - 1)\n",
    "                    match_count = 1\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names to use as lables\n",
    "cols = logs_df.columns.values.tolist()\n",
    "\n",
    "# do not use raw columns as labels\n",
    "cols.remove('raw')\n",
    "cols.remove('raw_preprocess')\n",
    "\n",
    "# using for loop for labeling funcition until string UDF capability in rapids- it is currently slow\n",
    "labels = []\n",
    "for indx in range(len(logs_df)):\n",
    "    labels.append(labeler(indx, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size\n",
    "Choose the maximum number of tokens and the overlap(stride) for your model. The tokenizer will split up logger logs and they will go through the model separately. There is a speed-tradeoff with smaller models inferencing faster, but potentially containing errors. You may need to experiment with these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "STRIDE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Labeling\n",
    "We are using the `bert-base-cased` tokenizer vocabulary. This tokenizer splits our whitespace separated words further into in dictionary sub-word pieces. The model eventually uses the label from the first piece of a word as the sole label for the word, so we do not care about the model's ability to predict individual labels for the sub-word pieces. For training, the label used for these pieces is `X`. To learn more see the [BERT paper](https://arxiv.org/abs/1810.04805)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword_labeler(tokenizer, log_list, label_list):\n",
    "    \"\"\"\n",
    "    label all non-initial subword pieces in tokenized log with an 'X'\n",
    "    \"\"\"\n",
    "    subword_labels = []\n",
    "    for log, tags in zip(log_list,label_list):\n",
    "        temp_tags = []\n",
    "        words = cudf.Series(log.split())\n",
    "        subword_counts = tokenizer(words,\n",
    "               max_length=10000,\n",
    "               max_num_rows=len(words),\n",
    "              add_special_tokens=False\n",
    "              )['metadata'][:,2]\n",
    "\n",
    "        for i, tag in enumerate(tags):\n",
    "            temp_tags.append(tag)\n",
    "            temp_tags.extend('X'* subword_counts[i].item())\n",
    "        if len(temp_tags) > MAX_SEQ_LEN:\n",
    "            split_temp_tags = [temp_tags[i:i+MAX_SEQ_LEN] for i in range(0, len(temp_tags)-STRIDE, MAX_SEQ_LEN-STRIDE)]\n",
    "            subword_labels.extend(split_temp_tags)\n",
    "        else:\n",
    "            subword_labels.append(temp_tags)\n",
    "    return subword_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.8/site-packages/cudf/core/subword_tokenizer.py:187: UserWarning: When truncation is not True, the behaviour currently differs from HuggingFace as cudf always returns overflowing tokens\n",
      "  warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "tokenizer  = SubwordTokenizer(\"resources/bert-base-cased-hash.txt\",do_lower_case=False)\n",
    "subword_labels = subword_labeler(tokenizer, logs_df.raw_preprocess.to_arrow().to_pylist(), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a set list of all labels from our dataset, add `X` for wordpiece tokens we will not have tags for and `[PAD]` for logs shorter than the length of the model's embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of labels\n",
    "label_values = list(set(x for l in labels for x in l))\n",
    "\n",
    "label_values[:0] = ['[PAD]']  \n",
    "\n",
    "# Set a dict for mapping id to tag name\n",
    "label2id = {t: i for i, t in enumerate(label_values)}\n",
    "label2id.update({'X': -100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(l, content, width):\n",
    "    l.extend([content] * (width - len(l)))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_labels = [pad(x[:MAX_SEQ_LEN], '[PAD]', MAX_SEQ_LEN) for x in subword_labels]\n",
    "int_labels = [[label2id.get(l) for l in lab] for lab in padded_labels]\n",
    "label_tensor = torch.tensor(int_labels).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Datasets\n",
    "For training and validation our datasets need three features. (1) `input_ids` subword tokens as integers padded to the specific length of the model (2) `attention_mask` a binary mask that allows the model to ignore padding (3) `labels` corresponding labels for tokens as integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer(logs_df.raw_preprocess,\n",
    "          max_length=MAX_SEQ_LEN,\n",
    "          stride = STRIDE,\n",
    "          truncation=False,\n",
    "          max_num_rows = len(logs_df.raw_preprocess)*3,\n",
    "          add_special_tokens=False,\n",
    "          return_tensors='pt'\n",
    "     )\n",
    "input_ids=output['input_ids'].type(torch.long)\n",
    "attention_masks=output['attention_mask'].type(torch.long)\n",
    "del output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pytorch random_split to create training and validation data subsets\n",
    "dataset_size = len(input_ids)\n",
    "train_size = int(dataset_size * 0.8)\n",
    "val_size = int(dataset_size - train_size)\n",
    "training_dataset, validation_dataset = random_split(dataset, (train_size, val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "train_dataloader = DataLoader(dataset=training_dataset, shuffle=True, batch_size=32)\n",
    "val_dataloader = DataLoader(dataset=validation_dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning pretrained BERT\n",
    "Download pretrained model from HuggingFace and move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label2id))\n",
    "\n",
    "# model to gpu\n",
    "model.cuda()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    # use multi-gpu\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define optimizer and learning rate for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    #fine tune all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [00:35<00:35, 35.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.18636336472931586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [01:10<00:00, 35.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0059268270875965185\n",
      "CPU times: user 44.8 s, sys: 25.7 s, total: 1min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# using 2 epochs to avoid overfitting\n",
    "\n",
    "epochs = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # TRAIN loop\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        # backward pass\n",
    "        loss.sum().backward()\n",
    "        # track train loss\n",
    "        tr_loss += loss.sum().item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no dropout or batch norm during eval\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.998655\n",
      "Accuracy score: 0.999771\n",
      "                                               precision    recall  f1-score   support\n",
      "\n",
      "                                  error_level      1.000     1.000     1.000       100\n",
      "                                error_message      1.000     1.000     1.000       100\n",
      "                                  remote_host      1.000     1.000     1.000       913\n",
      "                       request_header_referer      1.000     1.000     1.000       508\n",
      "                    request_header_user_agent      1.000     1.000     1.000      1002\n",
      "request_header_user_agent__os__version_string      0.875     1.000     0.933        14\n",
      "                             request_http_ver      1.000     1.000     1.000       913\n",
      "                               request_method      1.000     1.000     1.000       913\n",
      "                                  request_url      0.997     0.981     0.989       913\n",
      "                           response_bytes_clf      1.000     1.000     1.000       911\n",
      "                                       status      1.000     1.000     1.000       912\n",
      "                                time_received      1.000     1.000     1.000       985\n",
      "\n",
      "                                    micro avg      0.999     0.998     0.999      8184\n",
      "                                    macro avg      0.989     0.998     0.994      8184\n",
      "                                 weighted avg      0.999     0.998     0.999      8184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mapping id to label\n",
    "id2label={label2id[key] : key for key in label2id.keys()}\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for step, batch in enumerate(val_dataloader):\n",
    "    input_ids, input_mask, label_ids = batch\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=None,\n",
    "        attention_mask=input_mask,)\n",
    "        \n",
    "        # For eval mode, the first result of outputs is logits\n",
    "        logits = outputs[0] \n",
    "        \n",
    "    # Get NER predicted result\n",
    "    logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    # Get NER true result\n",
    "    label_ids = label_ids.detach().cpu().numpy()\n",
    "    \n",
    "    # Only predict the groud truth, mask=0, will not calculate\n",
    "    input_mask = input_mask.detach().cpu().numpy()\n",
    "    \n",
    "    # Compare the valuable predict result\n",
    "    for i,mask in enumerate(input_mask):\n",
    "        # ground truth \n",
    "        temp_1 = []\n",
    "        # Prediction\n",
    "        temp_2 = []\n",
    "        \n",
    "        for j, m in enumerate(mask):\n",
    "            # Mask=0 is PAD, do not compare\n",
    "            if m: # Exclude the X label\n",
    "                if id2label[label_ids[i][j]] != \"X\" and id2label[label_ids[i][j]] != \"[PAD]\": \n",
    "                    temp_1.append(id2label[label_ids[i][j]])\n",
    "                    temp_2.append(id2label[logits[i][j]])\n",
    "            else:\n",
    "                break      \n",
    "        y_true.append(temp_1)\n",
    "        y_pred.append(temp_2)\n",
    "\n",
    "print(\"f1 score: %f\"%(f1_score(y_true, y_pred)))\n",
    "print(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Get acc , recall, F1 result report\n",
    "print(classification_report(y_true, y_pred,digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model files for future parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.module.config.id2label = id2label\n",
    "    model.module.config.label2id = label2id\n",
    "    model.module.save_pretrained('log_parsing_apache_morpheus')\n",
    "else:\n",
    "    model.config.id2label = id2label\n",
    "    model.config.label2id = label2id\n",
    "    model.save_pretrained('log_parsing_apache_morpheus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we show an example of how you can quickly fine-tune a BERT model to parse cyber logs to create a more robust and flexible log parser as compared to traditional rules-based parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://developer.nvidia.com/blog/cybert-rapids-ai/\n",
    "\n",
    "https://medium.com/rapids-ai/cybert-28b35a4c81c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
