{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection pipeline using Graph Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content\n",
    "1. Introduction\n",
    "1. Load transaction data\n",
    "2. Graph construction\n",
    "3. GraphSage training\n",
    "5. Classifiction and Prediction\n",
    "6. Evaluation\n",
    "7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "This workflow shows an application of a graph neural network for fraud detection in a credit card transaction graph. We use a transaction dataset that includes three types of nodes, `transaction`, `client`, and `merchant` nodes. We use `GraphSAGE` along `XGBoost` to identify frauds in transactions. Since the graph is heterogeneous we employ HinSAGE a heterogeneous implementation of GraphSAGE.\n",
    "\n",
    "First, GraphSAGE is trained separately to produce embedding of transaction nodes, then the embedding is fed to `XGBoost` classifier to identify fraud and nonfraud transactions. During the inference stage,  an embedding for a new transaction is computed from the trained GraphSAGE model and then feed to XGBoost model to get the anomaly scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Credit Card Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "\n",
    "import dgl\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import HeteroRGCN\n",
    "from model import HinSAGE\n",
    "from model import prepare_data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from torchmetrics.functional import accuracy\n",
    "from tqdm import trange\n",
    "from training import build_fsi_graph\n",
    "from training import evaluate\n",
    "from training import get_metrics\n",
    "from training import init_loaders\n",
    "from training import save_model\n",
    "from training import train\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import cudf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1001)\n",
    "torch.manual_seed(1001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load traing and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace training-data.csv and validation-data.csv with training & validation csv in dataset file.\n",
    "TRAINING_DATA ='../../datasets/training-data/fraud-detection-training-data.csv'\n",
    "VALIDATION_DATA = '../../datasets/validation-data/fraud-detection-validation-data.csv'\n",
    "train_data = cudf.read_csv(TRAINING_DATA)\n",
    "inductive_data = cudf.read_csv(VALIDATION_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of samples of training data is small we augment data using benign transaction examples from the original training samples. This increases the number of benign example and reduce the proportion of fraudulent transactions. This is similar to practical situation where frauds are few in proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase number of samples.\n",
    "def augment_data(train_data=train_data, n=20):\n",
    "    train_data.drop(columns=['index'], inplace=True, axis=1)\n",
    "    non_fraud = train_data[train_data['fraud_label'] == 0]\n",
    "    df_fraud = cudf.concat([non_fraud for _ in range(n)])\n",
    "    df_train = cudf.concat([train_data, df_fraud])\n",
    "    df_train.reset_index(inplace=True)\n",
    "    df_train['index'] = df_train.index\n",
    "\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augment_data(train_data, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearage test data index\n",
    "last_train_index = train_data.index.max()+1\n",
    "inductive_data.index = np.arange(last_train_index, last_train_index + inductive_data.shape[0])\n",
    "inductive_data['index'] = inductive_data.index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_data` variable stores the data that will be used to construct graphs on which the representation learners can train. \n",
    "The `inductive_data` will be used to test the inductive performance of our representation learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The distribution of fraud for the train data is:\\n', train_data['fraud_label'].value_counts())\n",
    "print('The distribution of fraud for the inductive data is:\\n', inductive_data['fraud_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split trainig, testing dataset\n",
    "train_data, test_data, train_idx, inductive_idx, labels, df = prepare_data(train_data, inductive_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct transaction graph network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, nodes, edges, and features are passed to the `build_fsi_graph` method. Note that client and merchant node data are featurless, instead node embedding is used as a feature for these nodes. Therefore all the relevant transaction data resides at the transaction node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = [\"client_node\", \"merchant_node\", \"index\"]\n",
    "\n",
    "# Build graph\n",
    "whole_graph, feature_tensors = build_fsi_graph(df, meta_cols)\n",
    "train_graph, _ = build_fsi_graph(train_data, meta_cols)\n",
    "\n",
    "# Dataset\n",
    "feature_tensors = feature_tensors.float()\n",
    "train_idx = torch.from_dlpack(train_idx.values.toDlpack()).long()\n",
    "inductive_idx = torch.from_dlpack(inductive_idx.values.toDlpack()).long()\n",
    "labels = torch.from_dlpack(labels.toDlpack()).long()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train Heterogeneous GraphSAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HinSAGE, a heterogeneous graph implementation of the GraphSAGE framework is trained with user specified hyperparameters. The model train several GraphSAGE models on the type of relationship between different types of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "target_node = \"transaction\"\n",
    "epochs = 25\n",
    "in_size, hidden_size, out_size, n_layers,\\\n",
    "    embedding_size = 111, 64, 2, 2, 1\n",
    "batch_size = 256\n",
    "in_size, hidden_size, out_size, n_layers, embedding_size = 111, 64, 2, 2, 1\n",
    "hyperparameters = {\n",
    "    \"in_size\": in_size,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"out_size\": out_size,\n",
    "    \"n_layers\": n_layers,\n",
    "    \"embedding_size\": embedding_size,\n",
    "    \"target_node\": target_node,\n",
    "    \"epoch\": epochs\n",
    "}\n",
    "\n",
    "scale_pos_weight = (labels[train_idx].sum() / train_data.shape[0]).item()\n",
    "scale_pos_weight = torch.FloatTensor([scale_pos_weight, 1 - scale_pos_weight]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_loader, val_loader, test_loader = init_loaders(train_graph.to(\n",
    "    device), train_idx, test_idx=inductive_idx,\n",
    "    val_idx=inductive_idx, g_test=whole_graph, batch_size=batch_size)\n",
    "\n",
    "# Set model variables\n",
    "model = HinSAGE(train_graph, in_size, hidden_size, out_size, n_layers, embedding_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_func = nn.CrossEntropyLoss(weight=scale_pos_weight.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(epochs):\n",
    "\n",
    "    train_acc, loss = train(\n",
    "        model, loss_func, train_loader, labels, optimizer, feature_tensors,\n",
    "        target_node)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Accuracy: {train_acc} | Train Loss: {loss}\")\n",
    "    val_logits, val_seed, _ = evaluate(model, val_loader, feature_tensors, target_node)\n",
    "    val_accuracy = accuracy(val_logits.argmax(1), labels.long()[val_seed].cpu(), \"binary\").item()\n",
    "    val_auc = roc_auc_score(\n",
    "        labels.long()[val_seed].cpu().numpy(),\n",
    "        val_logits[:, 1].numpy(),\n",
    "    )\n",
    "    print(f\"Validation Accuracy: {val_accuracy} auc {val_auc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Inductive Step GraphSAGE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to compute the inductive embedding of a new transaction. To extract the embedding of the new transactions, we need to keep indices of the original graph nodes along with the new transaction nodes. We need to concatenate the test data frame to the train data frame to create a new graph that includes all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(whole_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inductive step applies the previously learned (and optimized) aggregation functions, part of the `trained_hinsage_model`. We also pass the new graph g_test, test data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "_, train_seeds, train_embedding = evaluate(model, train_loader, feature_tensors, target_node)\n",
    "test_logits, test_seeds, test_embedding = evaluate(model, test_loader, feature_tensors, target_node)\n",
    "\n",
    "# compute metrics\n",
    "test_acc = accuracy(test_logits.argmax(dim=1), labels.long()[test_seeds].cpu(), \"binary\").item()\n",
    "test_auc = roc_auc_score(labels.long()[test_seeds].cpu().numpy(), test_logits[:, 1].numpy())\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_acc} auc {test_auc}\")\n",
    "\n",
    "#acc, f_1, precision, recall, roc_auc, pr_auc, average_precision, _, _ = get_metrics(\n",
    "#    test_logits.numpy(), labels[test_seeds].cpu().numpy())\n",
    "\n",
    "#print(f\"Final Test Accuracy: {acc} auc {roc_auc}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Classification: predictions based on inductive embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a selected classifier (XGBoost) can be trained using the training node embedding and test on the test node embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier on embedding vector\n",
    "classifier = XGBClassifier(n_estimators=100)\n",
    "classifier.fit(train_embedding.cpu().numpy(), labels[train_seeds].cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If requested, the original transaction features are added to the generated embeddings. If these features are added, a baseline consisting of only these features (without embeddings) is included to analyze the net impact of embeddings on the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = classifier.predict_proba(test_embedding.cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the highly imbalanced nature of the dataset, we can evaluate the results based on AUC, accuracy ...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f_1, precision, recall, roc_auc, pr_auc, average_precision, _, _ = get_metrics(\n",
    "    xgb_pred, labels[inductive_idx].cpu().numpy(),  name='HinSAGE_XGB')\n",
    "print(f\"Final Test Accuracy: {acc} auc {roc_auc}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result shows, using GNN embedded features with XGB achieves with a better performance when tested over embedded features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphsage and xgboost models can be saved into their respective save format using `save_model` method. For infernce, graphsage load as pytorch model, and the XGBoost load using `cuml` *Forest Inference Library (FIL)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir= \"modelpath/\"\n",
    "\n",
    "save_model(train_graph, model, hyperparameters, classifier, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls modelpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For inference we can load from file as follows.\n",
    "from training import load_model\n",
    "\n",
    "# do inference on loaded model, as follows\n",
    "hinsage_model,  hyperparam, g = load_model(model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workflow, we show a hybrid approach how to use Graph Neural network along XGBoost for a fraud detection on credit card transaction network. For further, optimized inference pipeline refer to `Morpheus` inference pipeline of fraud detection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "1. Van Belle, RafaÃ«l, et al. \"Inductive Graph Representation Learning for fraud detection.\" Expert Systems with Applications (2022): 116463.\n",
    "2.https://stellargraph.readthedocs.io/en/stable/hinsage.html?highlight=hinsage\n",
    "3.https://github.com/rapidsai/clx/blob/branch-0.20/examples/forest_inference/xgboost_training.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
